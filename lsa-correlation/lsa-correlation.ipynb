{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (C) Egon Kidmose 2015-2017\n",
    "\n",
    "This file is part of lstm-rnn-correlation.\n",
    "\n",
    "lstm-rnn-correlation is free software: you can redistribute it and/or\n",
    "modify it under the terms of the GNU Lesser General Public License as\n",
    "published by the Free Software Foundation, either version 3 of the\n",
    "License, or (at your option) any later version.\n",
    "\n",
    "lstm-rnn-correlation is distributed in the hope that it will be\n",
    "useful, but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n",
    "Lesser General Public License for more details.\n",
    "\n",
    "You should have received a copy of the GNU Lesser General Public\n",
    "License along with lstm-rnn-correlation. If not, see\n",
    "<http://www.gnu.org/licenses/>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alert correlation with Laten Semantic Analysis\n",
    "\n",
    "Implemented as an example of an alternative to lstm-rnn-tied-weights.\n",
    "Aim is to be have input and output of the same format as lstm-rnn-tied-weights, in order to easy data preparation and calculation of metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import datetime\n",
    "import socket\n",
    "import json\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib\n",
    "try: # If X is not available select backend not requiring X\n",
    "    os.environ['DISPLAY']\n",
    "except KeyError:\n",
    "    matplotlib.use('Agg')\n",
    "try: # If ipython, do inline\n",
    "    %matplotlib inline\n",
    "except NameError:\n",
    "    pass\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger('lsa')\n",
    "logger.handlers = []\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "OUTPUT = 'output'\n",
    "runid = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "if os.environ.get('SLURM_ARRAY_TASK_ID', False):\n",
    "    runid += '-slurm-{}_{}'.format(\n",
    "        os.environ['SLURM_ARRAY_JOB_ID'],\n",
    "        os.environ['SLURM_ARRAY_TASK_ID']\n",
    "    )\n",
    "elif os.environ.get('SLURM_JOB_ID', False):\n",
    "    runid += '-slurm-' + os.environ['SLURM_JOB_ID']\n",
    "else:\n",
    "     runid += '-' + socket.gethostname()\n",
    "out_dir = OUTPUT + '/' + runid\n",
    "if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir)\n",
    "out_prefix = out_dir + '/' + runid + '-'\n",
    "\n",
    "# info log file\n",
    "infofh = logging.FileHandler(out_prefix + 'info.log')\n",
    "infofh.setLevel(logging.INFO)\n",
    "infofh.setFormatter(logging.Formatter(\n",
    "        fmt='%(message)s',\n",
    "))\n",
    "logger.addHandler(infofh)\n",
    "\n",
    "# verbose log file\n",
    "vfh = logging.FileHandler(out_prefix + 'verbose.log')\n",
    "vfh.setLevel(logging.DEBUG)\n",
    "vfh.setFormatter(logging.Formatter(\n",
    "        fmt='%(asctime)s - PID:%(process)d - %(levelname)s - %(message)s',\n",
    "))\n",
    "logger.addHandler(vfh)\n",
    "\n",
    "# Console logger\n",
    "ch = logging.StreamHandler()\n",
    "ch.setLevel(logging.INFO)\n",
    "ch.setFormatter(logging.Formatter())\n",
    "logger.addHandler(ch)\n",
    "\n",
    "logger.info('Output, including logs, are going to: {}'.format(out_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse env vars\n",
    "env = dict()\n",
    "\n",
    "# Data control\n",
    "env['CSVIN'] = os.environ.get('CSVIN', None)\n",
    "if env['CSVIN'] is None:\n",
    "    logger.critical(\"Cannot run without a CSV input\")\n",
    "    sys.exit(-1)\n",
    "env['RAND_SEED'] = int(os.environ.get('RAND_SEED', time.time())) # Current unix time if not specified\n",
    "env['VAL_CUT'] = int(os.environ.get('VAL_CUT', -1))\n",
    "if not (0 <= env['VAL_CUT']) and (env['VAL_CUT'] <= 9):\n",
    "    logger.critical(\"Invalid cross validation cut: {}\".format(env['VAL_CUT']))\n",
    "    sys.exit(-1)\n",
    "\n",
    "logger.info(\"Starting.\")\n",
    "logger.info(\"env: \" + str(env))\n",
    "for k in sorted(env.keys()):\n",
    "    logger.info('env[\\'{}\\']: {}'.format(k,env[k]))\n",
    "\n",
    "logger.debug('Saving env')\n",
    "with open(out_prefix + 'env.json', 'w') as f:\n",
    "    json.dump(env, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = env['RAND_SEED']\n",
    "def rndseed():\n",
    "    global seed\n",
    "    seed += 1\n",
    "    return seed\n",
    "\n",
    "def shuffle(pairs):\n",
    "    np.random.seed(rndseed())\n",
    "    return pairs.reindex(np.random.permutation(pairs.index))\n",
    "\n",
    "class Timer(object):\n",
    "    def __init__(self, name='', log=logger.info):\n",
    "        self.name = name\n",
    "        self.log = log\n",
    "        \n",
    "    def __enter__(self):\n",
    "        self.log('Timer(%s) started' % (self.name, ))\n",
    "        self.start = time.time()\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, *args):\n",
    "        self.end = time.time()\n",
    "        self.dur = datetime.timedelta(seconds=self.end - self.start)\n",
    "        self.log('Timer(%s):\\t%s' % (self.name, self.dur))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "logger.info('Loading data')\n",
    "data = pd.read_csv(env['CSVIN'])\n",
    "data.loc[data.incident == 'benign','incident'] = '-1' # encode benign with -1\n",
    "data.incident = data.incident.astype(int) # parse strings'\n",
    "\n",
    "idx_train = data.cut != env['VAL_CUT']\n",
    "idx_val = data.cut == env['VAL_CUT']\n",
    "\n",
    "logger.info(data.shape)\n",
    "logger.info(data.groupby(data.incident).incident.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build LSA pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectoriser\n",
    "vectoriser = TfidfVectorizer(\n",
    "    max_df=0.5, # ignore tokens occuring in more than # of alerts\n",
    "    max_features=10000, # only # most common tokens\n",
    "    min_df=2, # Only if token is in more than two alerts\n",
    "    analyzer='char', # Do N-gram analysis\n",
    "    ngram_range=(1, 3)\n",
    ")\n",
    "\n",
    "vectoriser.fit(data[idx_train].alert)\n",
    "\n",
    "logger.info(\n",
    "    \"Features in tf-idf vector: %d\" % \n",
    "    vectoriser.transform(data[idx_train].alert).get_shape()[1]\n",
    ")\n",
    "\n",
    "# SVD\n",
    "COMPONENTS = 100\n",
    "svd = TruncatedSVD(COMPONENTS)\n",
    "\n",
    "# lsa\n",
    "lsa = make_pipeline(\n",
    "    vectoriser,\n",
    "    svd, \n",
    "    Normalizer(copy=False)\n",
    ")\n",
    "\n",
    "# Fit pipeline\n",
    "_ = lsa.fit(data[idx_train].alert)\n",
    "\n",
    "logger.info(\n",
    "    \"Variance explained by SVD using %d components: %.3f %%\" %\n",
    "    (COMPONENTS, svd.explained_variance_ratio_.sum() * 100. )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tranform data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tranform data\n",
    "data = data.join(\n",
    "    pd.DataFrame(\n",
    "        lsa.transform(data.alert), \n",
    "        index=data.index,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precompute_distance_matrix(X):\n",
    "    \"\"\"\n",
    "    precomputing takes 20 sec/500 samples on js3, OMP_NUM_THREADS=16\n",
    "    precomputing takes 9 min/2632 samples on js3, OMP_NUM_THREADS=16\n",
    "    \"\"\"\n",
    "    return sp.spatial.distance.cdist(X, X, metric='cosine')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info('Getting alerts for clustering')\n",
    "clust_alerts = {\n",
    "    'train': (data[idx_train][list(range(100))].values, data[idx_train].incident.values),\n",
    "    'validation': (data[idx_val][list(range(100))].values, data[idx_val].incident.values), \n",
    "}\n",
    "\n",
    "logger.info(\"Precomputing clustering alert distances\")\n",
    "\n",
    "X = dict()\n",
    "X_dist_precomp = dict()\n",
    "y = dict()\n",
    "for (cut, v) in clust_alerts.items():\n",
    "    alerts_matrix, incidents_vector = v\n",
    "    X[cut] = alerts_matrix\n",
    "    X_dist_precomp[cut] = precompute_distance_matrix(X[cut])\n",
    "    y[cut] = incidents_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn import metrics\n",
    "\n",
    "def cluster(eps, min_samples, X_dist_precomp):\n",
    "    return DBSCAN(\n",
    "        eps=eps, min_samples=min_samples, metric='precomputed'\n",
    "    ).fit(X_dist_precomp)\n",
    "\n",
    "def build_cluster_to_incident_mapper(y, y_pred):\n",
    "    # Assign label to clusters according which incident has the largest part of its alert in the given cluster\n",
    "    # weight to handle class skew\n",
    "    weights = {l: 1/cnt for (l, cnt) in zip(*np.unique(y, return_counts=True))}\n",
    "    allocs = zip(y, y_pred)\n",
    "\n",
    "    from collections import Counter\n",
    "    c = Counter(map(tuple, allocs))\n",
    "\n",
    "    mapper = dict()\n",
    "    for _, (incident, cluster) in sorted([(c[k]*weights[k[0]], k) for k in c.keys()]):\n",
    "        mapper[cluster] = incident\n",
    "\n",
    "    mapper[-1] = -1 # Don't rely on what DBSCAN deems as noise\n",
    "    return mapper\n",
    "\n",
    "def dbscan_predict(dbscan_model, X_new, metric=sp.spatial.distance.cosine):\n",
    "    # Result is noise by default\n",
    "    y_new = np.ones(shape=len(X_new), dtype=int)*-1 \n",
    "\n",
    "    # Iterate all input samples for a label\n",
    "    for j, x_new in enumerate(X_new):\n",
    "        for i, x_core in enumerate(X['train'][dbscan_model.core_sample_indices_]): \n",
    "            if  metric(x_new, x_core) < dbscan_model.eps:\n",
    "                # Assign label of x_core to x_new\n",
    "                y_new[j] = dbscan_model.labels_[dbscan_model.core_sample_indices_[i]]\n",
    "                break\n",
    "    return y_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Iterating clustering algorithm parameters\")\n",
    "epss = np.array([0.0001, 0.0003, 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1])\n",
    "min_sampless = np.array([1, 3, 10, 30])\n",
    "\n",
    "def ParamSpaceMatrix(dtype=None):\n",
    "    return np.zeros(shape=(len(epss), len(min_sampless)), dtype=dtype)\n",
    "\n",
    "cl_model = ParamSpaceMatrix(dtype=object)\n",
    "mapper = ParamSpaceMatrix(dtype=object)\n",
    "\n",
    "cuts = ['train', 'validation']\n",
    "\n",
    "def ParamSpaceMatrices(dtype=None):\n",
    "    return {cut : ParamSpaceMatrix(dtype=dtype) for cut in cuts}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster and build mapper\n",
    "for i, eps in enumerate(epss):\n",
    "    for j, min_samples in enumerate(min_sampless):\n",
    "        logger.info(\"Clustering, eps={}, min_samples={}\".format(eps, min_samples))\n",
    "        # Cluster\n",
    "        cl_model[i,j] = cluster(eps, min_samples, X_dist_precomp['train'])\n",
    "        # get cluster assignments\n",
    "        y_pred = cl_model[i,j].labels_ \n",
    "        # Build classifier - get mapper used for classification\n",
    "        mapper[i,j] = build_cluster_to_incident_mapper(y['train'], y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict \n",
    "y_pred = ParamSpaceMatrices(dtype=object)\n",
    "y_pred_inc = ParamSpaceMatrices(dtype=object)\n",
    "\n",
    "for cut in cuts:\n",
    "    for i, eps in enumerate(epss):\n",
    "        for j, min_samples in enumerate(min_sampless):\n",
    "            if cut == 'train':\n",
    "                # pred is abused to hold clustering results\n",
    "                y_pred[cut][i,j] = cl_model[i,j].labels_ # cluster assignment\n",
    "                y_pred_inc[cut][i,j] = y[cut] # true incident label\n",
    "            elif cut == 'validation':\n",
    "                logger.info('Predicting for (eps, min_samples)=({:1.0e},{:>2d})'.format(eps, min_samples))\n",
    "                y_pred[cut][i,j] = dbscan_predict(cl_model[i][j], X[cut])\n",
    "                # START:DEBUG CODE\n",
    "                if not y[cut].shape == y_pred[cut][i,j].shape:\n",
    "                    raise ValueError(\n",
    "                        'y dimension must match. ({} != {})'.format(\n",
    "                            y.shape, y_pred.shape\n",
    "                    ))\n",
    "                # END:DEBUG CODE\n",
    "                y_pred_inc[cut][i,j] = np.array([mapper[i,j][el] for el in y_pred[cut][i,j]]) # predict incident\n",
    "            else:\n",
    "                raise NotImplementedError('Unexpected value for cut:{}'.format(cut))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def false_alert_rate_outliers_score(y, y_pred):\n",
    "    idx_outliers = y_pred == -1\n",
    "    return (y[idx_outliers] == -1).mean()\n",
    "\n",
    "def false_alert_rate_clusters_score(y, y_pred):\n",
    "    idx_clusters = y_pred != -1\n",
    "    return (y[idx_clusters] == -1).mean()\n",
    "\n",
    "def arf_score(y, y_pred):\n",
    "    # START:DEBUG CODE\n",
    "    if not len(set(y_pred)):\n",
    "        logger.critical('len(set(y_pred))={}'.format(len(set(y_pred))))\n",
    "        logger.critical('set(y_pred))={}'.format(set(y_pred)))\n",
    "        logger.critical('y_pred={}'.format(y_pred))\n",
    "    # END:DEBUG CODE\n",
    "    return len(y) / len(set(y_pred))\n",
    "\n",
    "def narf_score(y, y_pred):\n",
    "    return (len(y) / len(set(y_pred)) - 1) / (len(y) - 1)\n",
    "\n",
    "def imr_score(y, y_pred):\n",
    "    df_clustering = pd.DataFrame({\n",
    "        'cluster': y_pred,\n",
    "        'inc_true': y,\n",
    "    })\n",
    "    cluster_sizes = pd.DataFrame({'cluster_size': df_clustering[df_clustering.cluster != -1].groupby('cluster').size()})\n",
    "    df_clustering = pd.merge(df_clustering, cluster_sizes.reset_index(), on='cluster', how='outer')\n",
    "    # asuming one alert is picked at random from each cluster;\n",
    "    # probability that given alert is picked to represent the cluster it belongs to\n",
    "    df_clustering['alert_pick_prob'] = 1/df_clustering.cluster_size \n",
    "    # probability distribution of what incident a cluster will be asumed to represent\n",
    "    df_prob = pd.DataFrame(df_clustering.groupby(['cluster', 'inc_true']).sum().alert_pick_prob.rename('inc_hit'))\n",
    "    # probability that a given incident will not come out of a cluster\n",
    "    df_prob['inc_miss'] = 1 - df_prob.inc_hit.fillna(0)\n",
    "    assert (df_prob[df_prob.inc_miss < 0].inc_miss.abs() < 1e-12).all(), \"Error larger than 1e-12, still just imprecission?\"\n",
    "    df_prob = df_prob.abs()\n",
    "    # ... of any cluster\n",
    "    inc_miss_prob = df_prob.reset_index().groupby('inc_true').inc_miss.prod().rename('inc_miss_prob')\n",
    "    inc_miss_prob = inc_miss_prob[inc_miss_prob.index != -1] # Don't care about missing the noise pseudo-incident\n",
    "    return inc_miss_prob.sum() / df_clustering[df_clustering.inc_true != -1].inc_true.unique().shape[0]\n",
    "\n",
    "def cm_inc_clust(y, y_pred):\n",
    "    cm_inc_clust = pd.DataFrame(\n",
    "        metrics.confusion_matrix(y, y_pred),\n",
    "        index=sorted(set.union(set(y), set(y_pred))),\n",
    "        columns=sorted(set.union(set(y), set(y_pred))),\n",
    "    )\n",
    "    # drop dummy row for non-existing incident IDs\n",
    "    assert (cm_inc_clust.drop(list(set(y)), axis=0) == 0).values.all(), \"Non-empty row for invalid incident id\"\n",
    "    cm_inc_clust = cm_inc_clust.loc[sorted(list(set(y)))]\n",
    "\n",
    "    # drop dummy collumns for non-existing cluster IDs\n",
    "    assert (cm_inc_clust.drop(list(set(y_pred)), axis=1) == 0).values.all(), \"Non-empty collumn for invalid cluster id\"\n",
    "    cm_inc_clust = cm_inc_clust[sorted(list(set(y_pred)))]\n",
    "\n",
    "    cm_inc_clust.rename(index={-1: 'benign'}, inplace=True)\n",
    "    cm_inc_clust.rename(columns={-1: 'noise'}, inplace=True)\n",
    "    return cm_inc_clust\n",
    "\n",
    "def cm_inc_inc(y, y_pred_inc):\n",
    "    cm_inc_inc = pd.DataFrame(\n",
    "        metrics.confusion_matrix(y, y_pred_inc),\n",
    "        index=sorted(set.union(set(y), set(y_pred_inc))),\n",
    "        columns=sorted(set.union(set(y), set(y_pred_inc))),\n",
    "    )\n",
    "\n",
    "    cm_inc_inc.rename(index={-1: 'benign'}, inplace=True)\n",
    "    cm_inc_inc.rename(columns={-1: 'benign'}, inplace=True)\n",
    "    return cm_inc_inc\n",
    "\n",
    "def per_class_metrics(y, y_pred_inc):\n",
    "    d = dict(\n",
    "        zip(\n",
    "            ['precission', 'recall', 'f1', 'support'],\n",
    "            metrics.precision_recall_fscore_support(y, y_pred_inc)\n",
    "        )\n",
    "    )\n",
    "    cmat = metrics.confusion_matrix(y, y_pred_inc)\n",
    "    d['accuracy'] = cmat.diagonal()/cmat.sum(axis=1)\n",
    "    d['labels'] = sklearn.utils.multiclass.unique_labels(y, y_pred_inc)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating metrics\n",
    "m = {\n",
    "    # clustering\n",
    "    'n_clusters': ParamSpaceMatrices(dtype=int),\n",
    "    'homogenity': ParamSpaceMatrices(),\n",
    "    'outliers': ParamSpaceMatrices(dtype=int),\n",
    "    # classification, general\n",
    "    'accuracy': ParamSpaceMatrices(),\n",
    "    'precision': ParamSpaceMatrices(),\n",
    "    'recall': ParamSpaceMatrices(),\n",
    "    'f1': ParamSpaceMatrices(),\n",
    "    'per_class': ParamSpaceMatrices(dtype=object),\n",
    "    # correlating and filtering metrics\n",
    "    'arf': ParamSpaceMatrices(),\n",
    "    'narf': ParamSpaceMatrices(),\n",
    "    'imr': ParamSpaceMatrices(),\n",
    "    'faro': ParamSpaceMatrices(),\n",
    "    'farc': ParamSpaceMatrices(),\n",
    "    # confusion matrices\n",
    "    'cm_inc_clust': ParamSpaceMatrices(dtype=object),\n",
    "    'cm_inc_inc': ParamSpaceMatrices(dtype=object),\n",
    "}\n",
    "\n",
    "for cut in cuts:\n",
    "    for i, eps in enumerate(epss):\n",
    "        for j, min_samples in enumerate(min_sampless):\n",
    "            # clustering metrics\n",
    "            # Number of clusters in labels, ignoring noise if present.\n",
    "            m['n_clusters'][cut][i,j] = len(set(y_pred[cut][i,j])) - (1 if -1 in y_pred[cut][i,j] else 0)\n",
    "            m['outliers'][cut][i,j] = sum(y_pred[cut][i,j] == -1)\n",
    "            m['homogenity'][cut][i,j] = metrics.homogeneity_score(y[cut], y_pred[cut][i,j])\n",
    "            # classification metrics\n",
    "            m['accuracy'][cut][i,j] = metrics.accuracy_score(y[cut], y_pred_inc[cut][i,j])\n",
    "            m['precision'][cut][i,j] = metrics.precision_score(y[cut], y_pred_inc[cut][i,j], average='weighted')\n",
    "            m['recall'][cut][i,j] = metrics.recall_score(y[cut], y_pred_inc[cut][i,j], average='weighted')\n",
    "            m['f1'][cut][i,j] = metrics.f1_score(y[cut], y_pred_inc[cut][i,j], average='weighted')\n",
    "            m['per_class'][cut][i,j] = per_class_metrics(y[cut], y_pred_inc[cut][i,j])\n",
    "            # correlating and filtering\n",
    "            m['arf'][cut][i,j] = arf_score(y[cut], y_pred[cut][i,j])\n",
    "            m['narf'][cut][i,j] = narf_score(y[cut], y_pred[cut][i,j])\n",
    "            m['imr'][cut][i,j] = imr_score(y[cut], y_pred[cut][i,j])\n",
    "            m['faro'][cut][i,j] = false_alert_rate_outliers_score(y[cut], y_pred[cut][i,j])\n",
    "            m['farc'][cut][i,j] = false_alert_rate_clusters_score(y[cut], y_pred[cut][i,j])\n",
    "            # confusion matrices\n",
    "            m['cm_inc_clust'][cut][i,j] = cm_inc_clust(y[cut], y_pred[cut][i,j])\n",
    "            m['cm_inc_inc'][cut][i,j] = cm_inc_inc(y[cut], y_pred_inc[cut][i,j])\n",
    "            \n",
    "            logger.info(\n",
    "                \"Performance on {} cut with (eps, min_samples)=({:1.0e},{:>2d}): n_clusters={:>3d}, homogenity={:1.3f}, f1={:1.3f}, noise={:>3d}\".format(\n",
    "                    cut, eps, min_samples, \n",
    "                    m['n_clusters'][cut][i,j],\n",
    "                    m['homogenity'][cut][i,j],\n",
    "                    m['f1'][cut][i,j],\n",
    "                    m['outliers'][cut][i,j],\n",
    "                )\n",
    "            )\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_plot_prepare(\n",
    "    title,\n",
    "):\n",
    "    fig, ax = plt.subplots(figsize=(10,6))\n",
    "    ax.set_title(title)\n",
    "\n",
    "    ax.set_yscale('log')\n",
    "    ax.set_ylabel('min_samples')\n",
    "    ax.set_yticklabels(min_sampless)\n",
    "    ax.set_yticks(min_sampless)\n",
    "    ax.set_ylim(min_sampless[0]/3, min_sampless[-1]*3)\n",
    "\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_xlabel('eps')\n",
    "    ax.set_xticklabels(epss)\n",
    "    ax.set_xticks(epss)\n",
    "    ax.set_xlim(epss[0]/3,epss[-1]*3)\n",
    "    ax.xaxis.set_major_formatter(matplotlib.ticker.FormatStrFormatter('%.0e'))\n",
    "\n",
    "\n",
    "def param_plot_scatter(\n",
    "    data,\n",
    "    xcoords,\n",
    "    ycoords,\n",
    "\n",
    "):\n",
    "    # scaling\n",
    "    data = np.copy(data)\n",
    "    data = data-data.min() # Range to start at zero\n",
    "    data = data/data.max() # Range to end at one\n",
    "\n",
    "    coords = np.array([\n",
    "            (i, j)\n",
    "            for i in range(data.shape[0])\n",
    "            for j in range(data.shape[1])\n",
    "    ])\n",
    "\n",
    "    plt.scatter(\n",
    "        xcoords[coords[:,0]],\n",
    "        ycoords[coords[:,1]],\n",
    "        data[coords[:,0], coords[:,1]]*1000,\n",
    "        c='white',\n",
    "        alpha=0.5\n",
    "    )\n",
    "\n",
    "\n",
    "def param_plot_annotate(\n",
    "    data,\n",
    "    xcoords,\n",
    "    ycoords,\n",
    "    fmt='{}',\n",
    "):\n",
    "    coords = np.array([\n",
    "            (i, j)\n",
    "            for i in range(data.shape[0])\n",
    "            for j in range(data.shape[1])\n",
    "    ])\n",
    "        \n",
    "    for x, y, label in zip(\n",
    "        xcoords[coords[:,0]],\n",
    "        ycoords[coords[:,1]],\n",
    "        data[coords[:,0], coords[:,1]],\n",
    "    ):\n",
    "        plt.annotate(\n",
    "            fmt.format(label),\n",
    "            xy = (x, y), xytext = (0, 0),\n",
    "            textcoords = 'offset points', ha = 'center', va = 'center',\n",
    "        )\n",
    "\n",
    "\n",
    "def param_plot_save(filename):\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filename, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for label, values, fmt in [\n",
    "    ('Cluster count', m['n_clusters'], '{}'),\n",
    "    ('Cluster homogenity', m['homogenity'], '{:.2f}'),\n",
    "    ('Outliers', m['outliers'], '{}'),\n",
    "]:\n",
    "    for cut in cuts:\n",
    "        param_plot_prepare('{} by DBSCAN parameters ({} alerts)'.format(label, cut))\n",
    "        param_plot_scatter(values[cut], epss, min_sampless)\n",
    "        param_plot_annotate(values[cut], epss, min_sampless, fmt=fmt)\n",
    "        \n",
    "        param_plot_save(\n",
    "            '{}{}_{}.pdf'.format(\n",
    "                out_prefix,\n",
    "                label.replace('(','').replace(')','').replace(\" \", \"_\").lower(),\n",
    "                cut\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for label, values, fmt in [\n",
    "    ('Incident prediction accuracy', m['accuracy'], '{:.2f}'),\n",
    "    ('Incident prediction precision', m['precision'],  '{:.2f}'),\n",
    "    ('Incident prediction recall', m['recall'], '{:.2f}'),\n",
    "    ('Incident prediction F1 score', m['f1'], '{:.2f}'),\n",
    "]:\n",
    "    for cut in ['validation']: # training evaluated on true labels for clustering is of little use\n",
    "        param_plot_prepare('{} by DBSCAN parameters ({} alerts)'.format(label, cut))\n",
    "        param_plot_scatter(values[cut], epss, min_sampless)\n",
    "        param_plot_annotate(values[cut], epss, min_sampless, fmt=fmt)\n",
    "        \n",
    "        param_plot_save(\n",
    "            '{}{}_{}.pdf'.format(\n",
    "                out_prefix,\n",
    "                label.replace('(','').replace(')','').replace(\" \", \"_\").lower(),\n",
    "                cut\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label, values, fmt in [\n",
    "    ('Alert Reduction Factor', m['arf'], '{:.2f}'),\n",
    "    ('Normalised Alert Reduction Factor', m['narf'], '{:.2e}'),\n",
    "    ('Incident Miss Rate', m['imr'], '{:.2e}'),\n",
    "    ('False alerts rate among outliers', m['faro'], '{:.2f}'),\n",
    "]:\n",
    "    for cut in cuts:\n",
    "        param_plot_prepare('{} by DBSCAN parameters ({} alerts)'.format(label, cut))\n",
    "        param_plot_scatter(values[cut], epss, min_sampless)\n",
    "        param_plot_annotate(values[cut], epss, min_sampless, fmt=fmt)\n",
    "        \n",
    "        param_plot_save(\n",
    "            '{}{}_{}.pdf'.format(\n",
    "                out_prefix,\n",
    "                label.replace('(','').replace(')','').replace(\" \", \"_\").lower(),\n",
    "                cut\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info('One fold of cross validation completed')\n",
    "sys.exit(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
