{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (C) Egon Kidmose 2015-2017\n",
    "\n",
    "This file is part of lstm-rnn-correlation.\n",
    "\n",
    "lstm-rnn-correlation is free software: you can redistribute it and/or\n",
    "modify it under the terms of the GNU Lesser General Public License as\n",
    "published by the Free Software Foundation, either version 3 of the\n",
    "License, or (at your option) any later version.\n",
    "\n",
    "lstm-rnn-correlation is distributed in the hope that it will be\n",
    "useful, but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n",
    "Lesser General Public License for more details.\n",
    "\n",
    "You should have received a copy of the GNU Lesser General Public\n",
    "License along with lstm-rnn-correlation. If not, see\n",
    "<http://www.gnu.org/licenses/>.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning how to tie weights together\n",
    "The purpose of this notebook is to learn/demonstrate how the weights of two layers can be tied together, such that the weights are always the same.\n",
    "\n",
    "         IN\n",
    "       /    \\ \n",
    "     L1      L2\n",
    "     |        |\n",
    "    OUT1  == OUT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "import lasagne\n",
    "from lasagne.layers import *\n",
    "from lasagne.nonlinearities import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_dataset(\n",
    "    ntrain=100,\n",
    "    nval=100,\n",
    "    ntest=100,\n",
    "):\n",
    "    def get_xors(n):\n",
    "        inputs = np.random.randint(0, 2, (n, 2)).astype(bool)\n",
    "        return inputs, inputs[:,0]^inputs[:,1]\n",
    "    \n",
    "    X_train, y_train = get_xors(ntrain)\n",
    "    X_val, y_val = get_xors(nval)\n",
    "    X_test, y_test = get_xors(ntest)\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "def iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n",
    "    assert len(inputs) == len(targets)\n",
    "    if shuffle:\n",
    "        indices = np.arange(len(inputs))\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(inputs) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield inputs[excerpt], targets[excerpt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, y_train, X_val, y_val, X_test, y_test = load_dataset()\n",
    "\n",
    "# For unit testing\n",
    "# X_unit, y_unit, _, _, _, _ = load_dataset(2, min_digits=1, max_digits=1)\n",
    "X_unit = [ 'abcdef', 'abcdef', 'qwerty']\n",
    "X_unit = [[ord(c) for c in w] for w in X_unit]\n",
    "X_unit = np.array(X_unit, dtype='int8')\n",
    "print(X_unit)\n",
    "n_alerts_unit, l_alerts_unit = X_unit.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First line of network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_var = T.imatrix('inputs')\n",
    "target_var = T.imatrix('targets')\n",
    "\n",
    "# Input layer\n",
    "n_alerts = None\n",
    "l_alerts = None\n",
    "l_in = InputLayer(shape=(n_alerts, l_alerts), input_var=input_var, name='INPUT-LAYER') # \n",
    "\n",
    "# Test\n",
    "pred_unit = get_output(l_in, inputs={l_in: input_var}).eval(\n",
    "    {input_var: X_unit})\n",
    "assert (pred_unit == X_unit).all(), \"Unexpected output\"\n",
    "\n",
    "print(pred_unit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Embedding layer\n",
    "n_alphabet = 2**7 # All ASCII chars\n",
    "\n",
    "l_emb = EmbeddingLayer(l_in, n_alphabet, n_alphabet, \n",
    "                         W=np.eye(n_alphabet,dtype='int8'),\n",
    "                         name='EMBEDDING-LAYER')\n",
    "l_emb.params[l_emb.W].remove('trainable') # Fix weight\n",
    "\n",
    "# Test\n",
    "pred_unit = get_output(l_emb, inputs={l_in: input_var}).eval(\n",
    "    {input_var: X_unit})\n",
    "assert (np.argmax(pred_unit, axis=2) == X_unit).all()\n",
    "assert np.all(pred_unit.shape == (n_alerts_unit, l_alerts_unit, n_alphabet ))\n",
    "print(pred_unit.shape)\n",
    "print(pred_unit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Recurrent LSTM layer\n",
    "num_units = 10\n",
    "l_lstm = LSTMLayer(l_emb, num_units=num_units, name='LSTM-LAYER')\n",
    "pred_unit = get_output(l_lstm, inputs={l_in: input_var}).eval({input_var: X_unit})\n",
    "assert pred_unit.shape == (n_alerts_unit, l_alerts_unit, num_units), \"Unexpected dimensions\"\n",
    "\n",
    "# Test\n",
    "pred_unit = get_output(l_lstm, inputs={l_in: input_var}).eval({input_var: [[1],[1]]})\n",
    "assert np.all(pred_unit[0] == pred_unit[1]), \"Repeated alerts must produce the same\"\n",
    "\n",
    "pred_unit = get_output(l_lstm, inputs={l_in: input_var}).eval({input_var: [[1,1],[1,1]]})\n",
    "assert np.all(pred_unit[0] == pred_unit[1]), \"Repeated alerts must produce the same\"\n",
    "\n",
    "pred_unit = get_output(l_lstm, inputs={l_in: input_var}).eval({input_var: [[1,1],[0,1]]})\n",
    "assert np.all(pred_unit[0] != pred_unit[1]), \"Earlier must affect laters\"\n",
    "\n",
    "pred_unit = get_output(l_lstm, inputs={l_in: input_var}).eval({input_var: [[1,0],[1,1]]})\n",
    "assert np.all(pred_unit[0,0] == pred_unit[1,0]), \"Later must not affect earlier\"\n",
    "assert np.all(pred_unit[0,1] != pred_unit[1,1]), \"Current must make a difference\"\n",
    "\n",
    "net1 = l_lstm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clone line, tie weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create an identical test network, with tied weights\n",
    "net2 = None\n",
    "for l in get_all_layers(net1):\n",
    "    print(\"{} ({}):\".format(l.name, l))\n",
    "    if isinstance(l, InputLayer):\n",
    "        net2 = InputLayer(\n",
    "            shape=l.shape, \n",
    "            input_var=l.input_var, \n",
    "            name=l.name+'2',\n",
    "        )\n",
    "    elif isinstance(l, DenseLayer):\n",
    "        net2 = DenseLayer(\n",
    "            net2,\n",
    "            num_units=l.num_units,\n",
    "            W=l.W,\n",
    "            b=l.b,\n",
    "            nonlinearity=l.nonlinearity,\n",
    "            name=l.name+'2',\n",
    "        )\n",
    "    elif isinstance(l, EmbeddingLayer):\n",
    "        net2 = EmbeddingLayer(\n",
    "            net2,\n",
    "            l.input_size,\n",
    "            l.output_size,\n",
    "            W=l.W,\n",
    "            name=l.name+'2',\n",
    "        )\n",
    "    elif isinstance(l, LSTMLayer):\n",
    "        net2 = LSTMLayer(\n",
    "            net2,\n",
    "            l.num_units,\n",
    "            ingate=Gate(W_in=l.W_in_to_ingate, W_hid=l.W_hid_to_ingate, W_cell=l.W_cell_to_ingate, b=l.b_ingate, nonlinearity=l.nonlinearity_ingate),\n",
    "            forgetgate=Gate(W_in=l.W_in_to_forgetgate, W_hid=l.W_hid_to_forgetgate, W_cell=l.W_cell_to_forgetgate, b=l.b_forgetgate, nonlinearity=l.nonlinearity_forgetgate),\n",
    "            cell=Gate(W_in=l.W_in_to_cell, W_hid=l.W_hid_to_cell, W_cell=None, b=l.b_cell, nonlinearity=l.nonlinearity_cell),\n",
    "            outgate=Gate(W_in=l.W_in_to_outgate, W_hid=l.W_hid_to_outgate, W_cell=l.W_cell_to_outgate, b=l.b_outgate, nonlinearity=l.nonlinearity_outgate),\n",
    "            nonlinearity=l.nonlinearity,\n",
    "            cell_init=l.cell_init,\n",
    "            hid_init=l.hid_init,\n",
    "            backwards=l.backwards,\n",
    "            learn_init=l.learn_init,\n",
    "            peepholes=l.peepholes,\n",
    "            gradient_steps=l.gradient_steps,\n",
    "            grad_clipping=l.grad_clipping,\n",
    "            unroll_scan=l.unroll_scan,\n",
    "            precompute_input=l.precompute_input,\n",
    "            # mask_input=l.mask_input, # AttributeError: 'LSTMLayer' object has no attribute 'mask_input'\n",
    "            name=l.name+'2',\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\"Unhandled layer: {}\".format(l))\n",
    "    print(' - added layer: {} ({})'.format(get_all_layers(net2)[-1], get_all_layers(net2)[-1].name))\n",
    "\n",
    "# Test\n",
    "pred_unit1 = get_output(net1, inputs={net1: input_var}).eval({input_var: X_unit})\n",
    "pred_unit2 = get_output(net2, inputs={net2: input_var}).eval({input_var: X_unit})\n",
    "assert np.all(pred_unit == pred_unit), \"The two lines must output the same\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Training\n",
    "prediction = lasagne.layers.get_output(net)\n",
    "loss = lasagne.objectives.categorical_crossentropy(prediction, target_var)\n",
    "loss = loss.mean()\n",
    "params = lasagne.layers.get_all_params(net, trainable=True)\n",
    "updates = lasagne.updates.sgd(loss, params, learning_rate=0.1)\n",
    "\n",
    "# Testing\n",
    "test_prediction = lasagne.layers.get_output(test_net, deterministic=True)\n",
    "test_loss = lasagne.objectives.categorical_crossentropy(test_prediction,\n",
    "                                                        target_var)\n",
    "test_loss = test_loss.mean()\n",
    "test_acc = T.mean(T.eq(T.argmax(test_prediction, axis=1), target_var),\n",
    "                  dtype=theano.config.floatX)\n",
    "\n",
    "train_fn = theano.function([input_var, target_var], loss, updates=updates)\n",
    "val_fn = theano.function([input_var, target_var], [test_loss, test_acc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Starting training...\")\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = load_dataset(100,100,100)\n",
    "num_epochs = 5000\n",
    "for epoch in range(num_epochs):\n",
    "    train_err = 0\n",
    "    train_batches = 0\n",
    "    start_time = time.time()\n",
    "    for batch in iterate_minibatches(X_train, y_train, 100, shuffle=True):\n",
    "        inputs, targets = batch\n",
    "        train_err += train_fn(inputs, targets)\n",
    "        train_batches += 1\n",
    "\n",
    "    val_err = 0\n",
    "    val_acc = 0\n",
    "    val_batches = 0\n",
    "    for batch in iterate_minibatches(X_val, y_val, 100, shuffle=False):\n",
    "        inputs, targets = batch\n",
    "        err, acc = val_fn(inputs, targets)\n",
    "        val_err += err\n",
    "        val_acc += acc\n",
    "        val_batches += 1\n",
    "\n",
    "    \"\"\"print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "        epoch + 1, num_epochs, time.time() - start_time))\n",
    "    print(\"  training loss:\\t\\t{:.6f}\".format(train_err / train_batches))\n",
    "    print(\"  validation loss:\\t\\t{:.6f}\".format(val_err / val_batches))\n",
    "    print(\"  validation accuracy:\\t\\t{:.2f} %\".format(\n",
    "        val_acc / val_batches * 100))\"\"\"\n",
    "\n",
    "test_err = 0\n",
    "test_acc = 0\n",
    "test_batches = 0\n",
    "for batch in iterate_minibatches(X_test, y_test, 100, shuffle=False):\n",
    "    inputs, targets = batch\n",
    "    err, acc = val_fn(inputs, targets)\n",
    "    test_err += err\n",
    "    test_acc += acc\n",
    "    test_batches += 1\n",
    "print(\"Final results:\")\n",
    "print(\"  test loss:\\t\\t\\t{:.6f}\".format(test_err / test_batches))\n",
    "print(\"  test accuracy:\\t\\t{:.2f} %\".format(\n",
    "    test_acc / test_batches * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=1) \n",
    "print('trained network paramters:')\n",
    "for l in get_all_layers(net):\n",
    "    print(l)\n",
    "    print(' {}'.format(l.name))\n",
    "    for p in l.get_params():\n",
    "        print(' {}'.format(p))\n",
    "        print('  {}'.format(p.get_value()))\n",
    "\n",
    "\n",
    "print()\n",
    "\n",
    "print('test network paramters:')\n",
    "for l in get_all_layers(test_net):\n",
    "    print(l)\n",
    "    print(' {}'.format(l.name))\n",
    "    for p in l.get_params():\n",
    "        print(' {}'.format(p))\n",
    "        print('  {}'.format(p.get_value()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
