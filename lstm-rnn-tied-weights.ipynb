{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Copyright (C) Egon Kidmose 2015-2017\n",
    "\n",
    "This file is part of lstm-rnn-correlation.\n",
    "\n",
    "lstm-rnn-correlation is free software: you can redistribute it and/or\n",
    "modify it under the terms of the GNU Lesser General Public License as\n",
    "published by the Free Software Foundation, either version 3 of the\n",
    "License, or (at your option) any later version.\n",
    "\n",
    "lstm-rnn-correlation is distributed in the hope that it will be\n",
    "useful, but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n",
    "Lesser General Public License for more details.\n",
    "\n",
    "You should have received a copy of the GNU Lesser General Public\n",
    "License along with lstm-rnn-correlation. If not, see\n",
    "<http://www.gnu.org/licenses/>.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alert correlation with a Long Short-Term Memoroy (LSTM) Recurrent Neural Network(RNN) and cosine similarity\n",
    "\n",
    "**Author:** Egon Kidmose, egk@es.aau.dk\n",
    "\n",
    "In network security a common task is to detect network intrusions and for this purpose an Intrusion Detections System (IDS) can be used to raise alerts on suspicious network traffic.\n",
    "Snort, Suricata and Bro are examples of free and open source IDSs (Commercial products also exist).\n",
    "The alerts generally provides low level information such as recognition of strings that are known to be part of security exploits or anomalous connection rates for a host.\n",
    "By grouping alerts that are correlated into higher level events, false positives might be suppressed and attack scenarios becomes easier to recognise.\n",
    "This is a take on how to correlate IDS alerts to determine which belong in the same group.\n",
    "\n",
    "Alerts can be represented as log lines with various information such as time stamp, IP adresses, protocol information and a description of what triggered the alert.\n",
    "It is assumed that such a log lines hold the information needed to determine if two alerts are correlated or not.\n",
    "\n",
    "The input to the neural network will be two alerts and the output will indicate if they are correlated or not.\n",
    "In further detail the inputs is two strings of ASCII characters of variable length.\n",
    "For the output a Cosine Similarity layer is implemented and used to produce an output in the range [-1,1], with -1 meaning opposite, 0 meaning orthogonal and 1 meaning the same.\n",
    "\n",
    "For the hidden layers only a single layers of Long Short-Term Memory (LSTM) cells is used.\n",
    "It is an option to experiment with adding more.\n",
    "Being reccurrent, such a layer handles variable length input well.\n",
    "\n",
    "While it turned out to be to challenging to implement, the initial idea was to let the two inputs pass through LSTM layers with identical weights.\n",
    "The intent was to have them act as transformations into a space where cosine similarity could be used to measure similarity of the alerts.\n",
    "However I have not succeded at tying the weights together.\n",
    "As an alternative this might be achieved by using all training pairs in both original and swapped order.\n",
    "The intuition is that this leads to two identical layers, but intuition also suggest that this is highly ineffective.\n",
    "\n",
    "                      Output\n",
    "                        |\n",
    "    Cosine similarity   #\n",
    "                       / \\\n",
    "        LSTM layers   #   #\n",
    "                      |   |\n",
    "        \"alert number 1\"  |\n",
    "            \"alert number 2\"\n",
    "\n",
    "\n",
    "Reference: Huang, Po-Sen, et al. \"Learning deep structured semantic models for web search using clickthrough data.\" Proceedings of the 22nd ACM international conference on Conference on information & knowledge management. ACM, 2013.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "import logging\n",
    "logging.getLogger().handlers = []\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import glob\n",
    "import json\n",
    "import subprocess\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "import lasagne\n",
    "from lasagne.layers import *\n",
    "from lasagne.nonlinearities import *\n",
    "from lasagne.updates import *\n",
    "from lasagne.objectives import *\n",
    "\n",
    "import lstm_rnn_tied_weights\n",
    "from lstm_rnn_tied_weights import CosineSimilarityLayer\n",
    "from lstm_rnn_tied_weights import load, modify, split, pool, cross_join, limit\n",
    "from lstm_rnn_tied_weights import iterate_minibatches\n",
    "from lstm_rnn_tied_weights import mask_ips, mask_tss, uniquify_victim\n",
    "logger = lstm_rnn_tied_weights.logger\n",
    "\n",
    "runid = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "out_dir = 'output/' + runid\n",
    "if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir)\n",
    "out_prefix = out_dir + '/' + runid + '-'\n",
    "# info log file\n",
    "infofh = logging.FileHandler(out_prefix + 'info.log')\n",
    "infofh.setLevel(logging.INFO)\n",
    "infofh.setFormatter(logging.Formatter(\n",
    "        fmt='%(message)s',\n",
    "))\n",
    "logger.addHandler(infofh)\n",
    "# verbose log file\n",
    "vfh = logging.FileHandler(out_prefix + 'verbose.log')\n",
    "vfh.setLevel(logging.DEBUG)\n",
    "vfh.setFormatter(logging.Formatter(\n",
    "        fmt='%(asctime)s - PID:%(process)d - %(levelname)s - %(message)s',\n",
    "))\n",
    "logger.addHandler(vfh)\n",
    "\n",
    "env = dict()\n",
    "# git\n",
    "env['version'] = subprocess.check_output([\"git\", \"describe\"]).strip()\n",
    "if not isinstance(env['version'], str):\n",
    "    env['version'] = str(env['version'], \"UTF-8\")\n",
    "\n",
    "# OMP\n",
    "env['OMP_NUM_THREADS'] = os.environ.get('OMP_NUM_THREADS', str())\n",
    "\n",
    "# Masking/modifying\n",
    "env['MASKING'] = os.environ.get('MASKING', str())\n",
    "env['MASK_IP'] = 'ip' in env['MASKING'].lower()\n",
    "env['MASK_TS'] = 'ts' in env['MASKING'].lower()\n",
    "env['UNIQUIFY_VICTIM'] = 'true' in os.environ.get('UNIQUIFY_VICTIM', 'true').lower()\n",
    "\n",
    "# cutting\n",
    "env['CUT'] = os.environ.get('CUT', str())\n",
    "if 'none' in env['CUT'].lower():\n",
    "    env['CUT_NONE'] = True\n",
    "elif 'inc' in env['CUT'].lower():\n",
    "    env['CUT_INC'] = True\n",
    "elif 'alert' in env['CUT'].lower():\n",
    "    env['CUT_ALERT'] = True\n",
    "elif 'pair' in env['CUT'].lower():\n",
    "    env['CUT_PAIR'] = True\n",
    "else:\n",
    "    raise NotImplementedError(\"Pleas set CUT={none|inc|alert|pair} (CUT={})\".format(env['CUT']))\n",
    "\n",
    "# Data control\n",
    "env['MAX_PAIRS'] = int(os.environ.get('MAX_PAIRS', 1000000))\n",
    "env['BATCH_SIZE'] = int(os.environ.get('BATCH_SIZE', 10000))\n",
    "env['EPOCHS'] = int(os.environ.get('EPOCHS', 10))\n",
    "env['SPLIT'] = [int(el) for el in os.environ.get('SPLIT', '60,20,20').split(',')]\n",
    "\n",
    "# Metadata\n",
    "env['VICTIM_IP'] = '147.32.84.165'\n",
    "\n",
    "# Neural network\n",
    "env['NN_UNITS'] = [int(el) for el in os.environ.get('NN_UNITS', '10').split(',')]\n",
    "env['NN_LEARNING_RATE'] = float(os.environ.get('NN_LEARNING_RATE', '0.1'))\n",
    "\n",
    "logger.info(\"Starting.\")\n",
    "logger.info(\"env: \" + str(env))\n",
    "for k in sorted(env.keys()):\n",
    "    logger.info('env[\\'{}\\']: {}'.format(k,env[k]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Data for unit testing\n",
    "X_unit = ['abcdef', 'abcdef', 'qwerty']\n",
    "X_unit = [[ord(c) for c in w] for w in X_unit]\n",
    "X_unit = np.array(X_unit, dtype='int8')\n",
    "logger.debug(X_unit)\n",
    "n_alerts_unit, l_alerts_unit = X_unit.shape\n",
    "mask_unit = np.ones(X_unit.shape, dtype='int8')\n",
    "logger.debug(mask_unit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dimensions\n",
    "n_alerts = None\n",
    "l_alerts = None\n",
    "n_alphabet = 2**7 # All ASCII chars\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Symbolic variables\n",
    "input_var, input_var2 = T.imatrices('inputs', 'inputs2')\n",
    "mask_var, mask_var2 = T.matrices('masks', 'masks2')\n",
    "target_var = T.dvector('targets')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# First line\n",
    "l_in = InputLayer(shape=(n_alerts, l_alerts), input_var=input_var, name='INPUT-LAYER')\n",
    "l_emb = EmbeddingLayer(l_in, n_alphabet, n_alphabet,\n",
    "                         W=np.eye(n_alphabet),\n",
    "                         name='EMBEDDING-LAYER')\n",
    "l_emb.params[l_emb.W].remove('trainable') # Fix weight\n",
    "l_mask = InputLayer(shape=(n_alerts, l_alerts), input_var=mask_var, name='MASK-INPUT-LAYER')\n",
    "l_lstm = l_emb\n",
    "for i, num_units in enumerate(env['NN_UNITS']):\n",
    "    logger.info('Adding {} units for {} layer'.format(num_units, i))\n",
    "    l_lstm = LSTMLayer(l_lstm, num_units=num_units, name='LSTM-LAYER[{}]'.format(i), mask_input=l_mask)\n",
    "l_slice = SliceLayer(l_lstm, indices=-1, axis=1, name=\"SLICE-LAYER\") # Only last timestep\n",
    "net = l_slice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Test first line\n",
    "\n",
    "# Test InputLayer\n",
    "pred_unit = get_output(l_in, inputs={l_in: input_var}).eval(\n",
    "    {input_var: X_unit})\n",
    "assert (pred_unit == X_unit).all(), \"Unexpected output\"\n",
    "# Test EmbeddingLayer\n",
    "pred_unit = get_output(l_emb, inputs={l_in: input_var}).eval(\n",
    "    {input_var: X_unit})\n",
    "assert (np.argmax(pred_unit, axis=2) == X_unit).all()\n",
    "assert np.all(pred_unit.shape == (n_alerts_unit, l_alerts_unit, n_alphabet ))\n",
    "# Test LSTMLayer\n",
    "pred_unit = get_output(\n",
    "    l_lstm,\n",
    "    inputs={l_in: input_var, l_mask: mask_var}\n",
    ").eval({input_var: X_unit, mask_var: mask_unit})\n",
    "assert pred_unit.shape == (n_alerts_unit, l_alerts_unit, num_units), \"Unexpected dimensions\"\n",
    "pred_unit = get_output(\n",
    "    l_lstm,\n",
    "    inputs={l_in: input_var, l_mask: mask_var}\n",
    ").eval({input_var: [[1],[1]], mask_var: [[1],[1]]})\n",
    "assert np.all(pred_unit[0] == pred_unit[1]), \"Repeated alerts must produce the same\"\n",
    "pred_unit = get_output(\n",
    "    l_lstm,\n",
    "    inputs={l_in: input_var, l_mask: mask_var}\n",
    ").eval({input_var: [[1,1],[1,1]], mask_var: [[1,1],[1,1]]})\n",
    "assert np.all(pred_unit[0] == pred_unit[1]), \"Repeated alerts must produce the same\"\n",
    "pred_unit = get_output(\n",
    "    l_lstm,\n",
    "    inputs={l_in: input_var, l_mask: mask_var}\n",
    ").eval({input_var: [[1,1],[0,1]], mask_var: [[1,1],[1,1]]})\n",
    "assert np.all(pred_unit[0] != pred_unit[1]), \"Earlier must affect laters\"\n",
    "pred_unit = get_output(\n",
    "    l_lstm,\n",
    "    inputs={l_in: input_var, l_mask: mask_var}\n",
    ").eval({input_var: [[1,0],[1,1]], mask_var: [[1,1],[1,1]]})\n",
    "assert np.all(pred_unit[0,0] == pred_unit[1,0]), \"Later must not affect earlier\"\n",
    "assert np.all(pred_unit[0,1] != pred_unit[1,1]), \"Current must make a difference\"\n",
    "# Test SliceLayer\n",
    "pred_unit = get_output(\n",
    "    l_slice,\n",
    "    inputs={l_in: input_var, l_mask: mask_var}\n",
    ").eval({input_var: X_unit, mask_var: mask_unit})\n",
    "assert pred_unit.shape == (n_alerts_unit, num_units), \"Unexpected shape\"\n",
    "pred_unit_lstm = get_output(\n",
    "    l_lstm,\n",
    "    inputs={l_in: input_var, l_mask: mask_var}\n",
    ").eval({input_var: X_unit, mask_var: mask_unit})\n",
    "assert np.all(pred_unit_lstm[:, -1, :] == pred_unit), \"Unexpected result of slicing\"\n",
    "\n",
    "logger.debug('OK')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Second line as a copy with shared weights\n",
    "l_in2 = InputLayer(shape=l_in.shape, input_var=input_var2, name=l_in.name+'2')\n",
    "l_mask2 = InputLayer(shape=l_mask.shape, input_var=mask_var2, name=l_mask.name+'2')\n",
    "net2 = lstm_rnn_tied_weights.clone(net, l_in2, l_mask2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Merge lines\n",
    "l_cos = CosineSimilarityLayer(net, net2, name=\"COSINE-SIMILARITY-LAYER\")\n",
    "l_sig = NonlinearityLayer(l_cos, nonlinearity=sigmoid, name=\"SIGMOID-LAYER\")\n",
    "cos_net = l_sig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Training Procedure\n",
    "t = time.time()\n",
    "prediction = get_output(cos_net)\n",
    "loss = binary_crossentropy(prediction, target_var)\n",
    "loss = loss.mean()\n",
    "params = get_all_params(cos_net, trainable=True)\n",
    "updates = sgd(loss, params, learning_rate=env['NN_LEARNING_RATE'])\n",
    "\n",
    "# Testing Procedure\n",
    "test_prediction = get_output(cos_net, deterministic=True)\n",
    "test_loss = binary_crossentropy(test_prediction, target_var)\n",
    "test_loss = test_loss.mean()\n",
    "test_acc = T.mean(T.eq(test_prediction > 0.5, target_var),\n",
    "                  dtype=theano.config.floatX)\n",
    "\n",
    "train_fn = theano.function([input_var, input_var2, mask_var, mask_var2, target_var], loss, updates=updates)\n",
    "val_fn = theano.function([input_var, input_var2, mask_var, mask_var2, target_var], [test_loss, test_acc])\n",
    "prediction_fn = theano.function([input_var, input_var2, mask_var, mask_var2], prediction)\n",
    "logger.debug(\"Spent {}s compilling.\".format(time.time()-t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alert_to_vector = theano.function([input_var, mask_var], get_output(l_slice))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def break_down_data(labels):\n",
    "    u, c = np.unique(labels, return_counts=True)\n",
    "    c_norm = (c/sum(c))*100\n",
    "    result = str()\n",
    "    result += 'Label: '+('{: >10}'*len(u)).format(*u) + '\\n'\n",
    "    result += 'Count: '+('{: >10}'*len(u)).format(*c) + '\\n'\n",
    "    result += 'Norm.: '+('{: >9.2f}%'*len(u)).format(*c_norm)\n",
    "    return result\n",
    "\n",
    "# If/what to mask out or modify\n",
    "modifier_fns = []\n",
    "if env['MASK_IP']:\n",
    "    modifier_fns.append(mask_ips)\n",
    "if env['MASK_TS']:\n",
    "    modifier_fns.append(mask_tss)\n",
    "if env['UNIQUIFY_VICTIM']:\n",
    "    modifier_fns.append(lambda incidents: uniquify_victim(incidents, env['VICTIM_IP']))\n",
    "\n",
    "def _get_batch(\n",
    "        alerts,\n",
    "        max_pairs,\n",
    "        offset=0,\n",
    "):\n",
    "    for sample in limit(\n",
    "            cross_join(alerts, offset=offset),\n",
    "            max_pairs,\n",
    "    ):\n",
    "        yield sample\n",
    "\n",
    "incidents = load([\n",
    "#'data/mcfp.felk.cvut.cz/publicDatasets/CTU-Malware-Capture-Botnet-132-1/2015-09-09_win3.pcap.shifted.out', #\t1\n",
    "#'data/mcfp.felk.cvut.cz/publicDatasets/CTU-Malware-Capture-Botnet-114-2/2015-04-22_capture-win2.pcap.shifted.out', #\t1\n",
    "#'data/mcfp.felk.cvut.cz/publicDatasets/CTU-Malware-Capture-Botnet-22/2013-11-06_capture-win8.pcap.shifted.out', #\t1\n",
    "#'data/mcfp.felk.cvut.cz/publicDatasets/CTU-Malware-Capture-Botnet-121-1/2015-04-22_capture-win5.pcap.shifted.out', #\t1\n",
    "#'data/mcfp.felk.cvut.cz/publicDatasets/CTU-Malware-Capture-Botnet-15/2013-09-28_capture-win19.pcap.shifted.out', #\t2\n",
    "#'data/mcfp.felk.cvut.cz/publicDatasets/CTU-Malware-Capture-Botnet-129-1/2015-06-30_capture-win20.pcap.shifted.out', #\t3\n",
    "#'data/mcfp.felk.cvut.cz/publicDatasets/CTU-Malware-Capture-Botnet-45/botnet-capture-20110815-rbot-dos-icmp.pcap.shifted.out', #\t3\n",
    "#'data/mcfp.felk.cvut.cz/publicDatasets/CTU-Malware-Capture-Botnet-143-1/2015-10-23_win6.pcap.shifted.out', #\t3\n",
    "#'data/mcfp.felk.cvut.cz/publicDatasets/CTU-Malware-Capture-Botnet-59/2014-03-12_capture-win15.pcap.shifted.out', #\t4\n",
    "#'data/mcfp.felk.cvut.cz/publicDatasets/CTU-Malware-Capture-Botnet-45/botnet-capture-20110815-rbot-dos-icmp-more-bandwith.pcap.shifted.out', #\t4\n",
    "#'data/mcfp.felk.cvut.cz/publicDatasets/CTU-Malware-Capture-Botnet-123-1/2015-04-22_capture-win8.pcap.shifted.out', #\t4\n",
    "#'data/mcfp.felk.cvut.cz/publicDatasets/CTU-Malware-Capture-Botnet-14/2013-10-18_capture-win15.pcap.shifted.out', #\t5\n",
    "#'data/mcfp.felk.cvut.cz/publicDatasets/CTU-Malware-Capture-Botnet-45/botnet-capture-20110815-rbot-dos.pcap.shifted.out', #\t5\n",
    "#'data/mcfp.felk.cvut.cz/publicDatasets/CTU-Malware-Capture-Botnet-118-1/2015-04-20_capture-win5.pcap.shifted.out', #\t6\n",
    "#'data/mcfp.felk.cvut.cz/publicDatasets/CTU-Malware-Capture-Botnet-92/192.168.3.104-eldorado2-1.pcap.shifted.out', #\t6\n",
    "#'data/mcfp.felk.cvut.cz/publicDatasets/CTU-Malware-Capture-Botnet-48/botnet-capture-20110816-sogou.pcap.shifted.out', #\t10\n",
    "#'data/mcfp.felk.cvut.cz/publicDatasets/CTU-Malware-Capture-Botnet-60/2014-03-12_win20.pcap.shifted.out', #\t12\n",
    "#'data/mcfp.felk.cvut.cz/publicDatasets/CTU-Malware-Capture-Botnet-11/capture-win19.pcap.shifted.out', #\t13\n",
    "#'data/mcfp.felk.cvut.cz/publicDatasets/CTU-Malware-Capture-Botnet-102/capture-win2.pcap.shifted.out', #\t13\n",
    "#'data/mcfp.felk.cvut.cz/publicDatasets/CTU-Malware-Capture-Botnet-90/192.168.3.104-unvirus.pcap.shifted.out', #\t18\n",
    "#'data/mcfp.felk.cvut.cz/publicDatasets/CTU-Malware-Capture-Botnet-134-1/2015-10-11_win3.pcap.shifted.out', #\t20\n",
    "#'data/mcfp.felk.cvut.cz/publicDatasets/CTU-Malware-Capture-Botnet-26/2013-10-30_capture-win10.pcap.shifted.out', #\t21\n",
    "#'data/mcfp.felk.cvut.cz/publicDatasets/CTU-Malware-Capture-Botnet-114-1/2015-04-09_capture-win2.pcap.shifted.out', #\t22\n",
    "#'data/mcfp.felk.cvut.cz/publicDatasets/CTU-Malware-Capture-Botnet-73/2014-05-16_capture-win15.pcap.shifted.out', #\t28\n",
    "#'data/mcfp.felk.cvut.cz/publicDatasets/CTU-Malware-Capture-Botnet-24/2013-11-06_capture-win18.pcap.shifted.out', #\t29\n",
    "#'data/mcfp.felk.cvut.cz/publicDatasets/CTU-Malware-Capture-Botnet-142-1/2015-10-23_win7.pcap.shifted.out', #\t85\n",
    "#'data/mcfp.felk.cvut.cz/publicDatasets/CTU-Malware-Capture-Botnet-47/botnet-capture-20110816-donbot.pcap.shifted.out', #\t88\n",
    "#'data/mcfp.felk.cvut.cz/publicDatasets/CTU-Malware-Capture-Botnet-65/2014-04-07_capture-win11.pcap.shifted.out', #\t90\n",
    "#'data/mcfp.felk.cvut.cz/publicDatasets/CTU-Malware-Capture-Botnet-46/botnet-capture-20110815-fast-flux.pcap.shifted.out', #\t100\n",
    "#'data/mcfp.felk.cvut.cz/publicDatasets/CTU-Malware-Capture-Botnet-113-1/2015-03-12_capture-win6.pcap.shifted.out', #\t184\n",
    "#'data/mcfp.felk.cvut.cz/publicDatasets/CTU-Malware-Capture-Botnet-2/2013-08-20_capture-win2.pcap.shifted.out', #\t317\n",
    "#'data/mcfp.felk.cvut.cz/publicDatasets/CTU-Malware-Capture-Botnet-116-1/2012-05-25-capture-1.pcap.shifted.out', #\t328\n",
    "#'data/mcfp.felk.cvut.cz/publicDatasets/CTU-Malware-Capture-Botnet-89-1/2014-09-15_capture-win2.pcap.shifted.out', #\t390\n",
    "#'data/mcfp.felk.cvut.cz/publicDatasets/CTU-Malware-Capture-Botnet-36/capture-win2.pcap.shifted.out', #\t395\n",
    "#'data/mcfp.felk.cvut.cz/publicDatasets/CTU-Malware-Capture-Botnet-49/botnet-capture-20110816-qvod.pcap.shifted.out', #\t444\n",
    "#'data/mcfp.felk.cvut.cz/publicDatasets/CTU-Malware-Capture-Botnet-128-1/2015-06-07_capture-win12.pcap.shifted.out', #\t611\n",
    "#'data/mcfp.felk.cvut.cz/publicDatasets/CTU-Malware-Capture-Botnet-140-1/2015-10-23_win11.pcap.shifted.out', #\t839\n",
    "#'data/mcfp.felk.cvut.cz/publicDatasets/CTU-Malware-Capture-Botnet-42/botnet-capture-20110810-neris.pcap.shifted.out', #\t865\n",
    "#'data/mcfp.felk.cvut.cz/publicDatasets/CTU-Malware-Capture-Botnet-55/capture-win13.pcap.shifted.out', #\t954\n",
    "'data/mcfp.felk.cvut.cz/publicDatasets/CTU-Malware-Capture-Botnet-140-2/2015-10-27_capture-win11.pcap.shifted.out', #\t1354\n",
    "'data/mcfp.felk.cvut.cz/publicDatasets/CTU-Malware-Capture-Botnet-141-1/2015-23-10_win10.pcap.shifted.out', #\t1548\n",
    "'data/mcfp.felk.cvut.cz/publicDatasets/CTU-Malware-Capture-Botnet-69/2014-04-07_capture-win17.pcap.shifted.out', #\t1704\n",
    "'data/mcfp.felk.cvut.cz/publicDatasets/CTU-Malware-Capture-Botnet-43/botnet-capture-20110811-neris.pcap.shifted.out', #\t1785\n",
    "#'data/mcfp.felk.cvut.cz/publicDatasets/CTU-Malware-Capture-Botnet-54/botnet-capture-20110815-fast-flux-2.pcap.shifted.out', #\t2015\n",
    "#'data/mcfp.felk.cvut.cz/publicDatasets/CTU-Malware-Capture-Botnet-100/2014-12-20_capture-win5.pcap.shifted.out', #\t2685\n",
    "#'data/mcfp.felk.cvut.cz/publicDatasets/CTU-Malware-Capture-Botnet-35-1/2014-01-31_capture-win7.pcap.shifted.out', #\t3199\n",
    "#'data/mcfp.felk.cvut.cz/publicDatasets/CTU-Malware-Capture-Botnet-149-2/2015-12-09_capture-win4.pcap.shifted.out', #\t3817\n",
    "#'data/mcfp.felk.cvut.cz/publicDatasets/CTU-Malware-Capture-Botnet-127-1/2015-06-07_capture-win8.pcap.shifted.out', #\t4900\n",
    "#'data/mcfp.felk.cvut.cz/publicDatasets/CTU-Malware-Capture-Botnet-44/botnet-capture-20110812-rbot.pcap.shifted.out', #\t5338\n",
    "#'data/mcfp.felk.cvut.cz/publicDatasets/CTU-Malware-Capture-Botnet-149-1/2015-12-09_capture-win4.pcap.shifted.out', #\t5896\n",
    "#'data/mcfp.felk.cvut.cz/publicDatasets/CTU-Malware-Capture-Botnet-126-1/2015-06-07_capture-win7.pcap.shifted.out', #\t6992\n",
    "#'data/mcfp.felk.cvut.cz/publicDatasets/CTU-Malware-Capture-Botnet-125-1/2015-06-07_capture-win5.pcap.shifted.out', #\t7461\n",
    "#'data/mcfp.felk.cvut.cz/publicDatasets/CTU-Malware-Capture-Botnet-110-4/2015-04-22_capture-win9.pcap.shifted.out', #\t17501\n",
    "#'data/mcfp.felk.cvut.cz/publicDatasets/CTU-Malware-Capture-Botnet-150-1/2015-12-05_capture-win3.pcap.shifted.out', #\t18854\n",
    "#'data/mcfp.felk.cvut.cz/publicDatasets/CTU-Malware-Capture-Botnet-3/2013-08-20_capture-win15.pcap.shifted.out', #\t38279\n",
    "#'data/mcfp.felk.cvut.cz/publicDatasets/CTU-Malware-Capture-Botnet-78-1/2014-05-30_capture-win8.pcap.shifted.out', #\t84863\n",
    "])\n",
    "incidents = modify(incidents, modifier_fns)\n",
    "alerts = pool(incidents)\n",
    "\n",
    "pair_cnt = min([env['MAX_PAIRS'], len(alerts)**2])\n",
    "logger.info('Maximum possible pairs; pair_cnt={} env[\\'MAX_PAIRS\\']={}, len(alerts)**2={}'.format(\n",
    "        pair_cnt,\n",
    "        env['MAX_PAIRS'],\n",
    "        len(alerts)**2,\n",
    "    ))\n",
    "maxes = (np.array(env['SPLIT'])/sum(env['SPLIT'])*pair_cnt).astype(int)\n",
    "logger.info('train, val and test max pairs: {}'.format(maxes))\n",
    "train_max, val_max, test_max = maxes\n",
    "logger.info('Breakdown of original data:\\n'+break_down_data([i[0] for i in pool(incidents)])+'\\n')\n",
    "\n",
    "if env.get('CUT_NONE', False):\n",
    "    get_train_batch = lambda: _get_batch(alerts, train_max)\n",
    "    get_val_batch = lambda: _get_batch(alerts, val_max)\n",
    "    get_test_batch = lambda: _get_batch(alerts, test_max)\n",
    "\n",
    "elif env.get('CUT_INC', False):\n",
    "    incidents = split(incidents, env['SPLIT'])\n",
    "    alerts_train, alerts_val, alerts_test = tuple(map(pool, incidents))\n",
    "\n",
    "    get_train_batch = lambda: _get_batch(alerts_train, train_max)\n",
    "    get_val_batch = lambda: _get_batch(alerts_val, val_max)\n",
    "    get_test_batch = lambda: _get_batch(alerts_test, test_max)\n",
    "\n",
    "elif env.get('CUT_ALERT', False):\n",
    "    alerts_train, alerts_val, alerts_test = split(alerts, env['SPLIT'])\n",
    "\n",
    "    get_train_batch = lambda: _get_batch(alerts_train, train_max)\n",
    "    get_val_batch = lambda: _get_batch(alerts_val, val_max)\n",
    "    get_test_batch = lambda: _get_batch(alerts_test, test_max)\n",
    "\n",
    "elif env.get('CUT_PAIR', False):\n",
    "    get_train_batch = lambda: _get_batch(alerts, train_max, offset=0)\n",
    "    get_val_batch = lambda: _get_batch(alerts, val_max, offset=train_max)\n",
    "    get_test_batch = lambda: _get_batch(alerts, test_max, offset=train_max+val_max)\n",
    "\n",
    "else:\n",
    "    raise NotImplementedError(\"No cut selected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "logger.debug('Loading model')\n",
    "with open('model.log') as f:\n",
    "    model = json.loads(f.read())\n",
    "\n",
    "keys = get_all_params(cos_net)\n",
    "keys = [str(k).replace('[0]','') for k in keys]\n",
    "\n",
    "params = [np.array(model['model'][k]) for k in keys]\n",
    "set_all_param_values(cos_net, params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a1, a2, m1, m2, cor, inc1, inc2 = range(7)\n",
    "for cut, batch_fn in [\n",
    "    ('training', get_train_batch),\n",
    "    ('validation', get_val_batch),\n",
    "    ('testing', get_test_batch),\n",
    "]:\n",
    "    logger.info('Breakdown of {} data;\\n'.format(cut))\n",
    "    logger.info('correlation:\\n'+break_down_data([p[cor] for p in batch_fn()]))\n",
    "    logger.info('incident 1:\\n'+break_down_data([p[inc1] for p in batch_fn()]))\n",
    "    logger.info('incident 2:\\n'+break_down_data([p[inc2] for p in batch_fn()])+'\\n')\n",
    "\n",
    "logger.info(\"Starting training...\")\n",
    "for epoch in range(env['EPOCHS']):\n",
    "    train_err = 0\n",
    "    train_batches = 0\n",
    "    start_time = time.time()\n",
    "    for batch in iterate_minibatches(get_train_batch(), env['BATCH_SIZE']):\n",
    "        train_err += train_fn(*batch)\n",
    "        train_batches += 1\n",
    "        logger.debug('Batch complete')\n",
    "\n",
    "    #if (epoch+1) % (env['EPOCHS']/10) == 0:\n",
    "    if True:\n",
    "        val_err = 0\n",
    "        val_acc = 0\n",
    "        val_batches = 0\n",
    "        for batch in iterate_minibatches(get_val_batch(), env['BATCH_SIZE']):\n",
    "            err, acc = val_fn(*batch)\n",
    "            val_err += err\n",
    "            val_acc += acc\n",
    "            val_batches += 1\n",
    "\n",
    "        logger.info(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "            epoch + 1, env['EPOCHS'], time.time() - start_time))\n",
    "        logger.info(\"  training loss:\\t\\t{:.20f}\".format(train_err / train_batches))\n",
    "        logger.info(\"  validation loss:\\t\\t{:.6f}\".format(val_err / val_batches))\n",
    "        logger.info(\"  validation accuracy:\\t\\t{:.2f} %\".format(\n",
    "            val_acc / val_batches * 100))\n",
    "\n",
    "    logger.debug('Epoch complete')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_err = 0\n",
    "test_acc = 0\n",
    "test_batches = 0\n",
    "for batch in iterate_minibatches(get_test_batch(), env['BATCH_SIZE']):\n",
    "    err, acc = val_fn(*batch)\n",
    "    test_err += err\n",
    "    test_acc += acc\n",
    "    test_batches += 1\n",
    "\n",
    "logger.info(\"Final results:\")\n",
    "logger.info(\"  test loss:\\t\\t\\t{:.6f}\".format(test_err / test_batches))\n",
    "logger.info(\"  test accuracy:\\t\\t{:.2f} %\".format(\n",
    "    test_acc / test_batches * 100))\n",
    "\n",
    "model = {'model':{str(p): v.tolist() for p, v in zip(get_all_params(cos_net), get_all_param_values(cos_net))}}\n",
    "model_str = json.dumps(model)\n",
    "logger.debug('Dumping model parameters:')\n",
    "logger.debug(model_str)\n",
    "\n",
    "\n",
    "\n",
    "logger.info('Completed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# might, might not have x available\n",
    "try:\n",
    "    os.environ['DISPLAY']\n",
    "except KeyError:\n",
    "    import matplotlib\n",
    "    matplotlib.use('Agg')\n",
    "# might, might not be notebook\n",
    "try:\n",
    "    %matplotlib inline\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "error_dict = dict()\n",
    "\n",
    "land = np.logical_and\n",
    "lnot = np.logical_not\n",
    "    \n",
    "for batch in iterate_minibatches(get_test_batch(), test_max, keep_incidents=True):\n",
    "    alerts1, alerts2, masks1, masks2, corelations, iz, js = batch\n",
    "    pred_floats = prediction_fn(alerts1, alerts2, masks1, masks2)\n",
    "\n",
    "    logger.debug(\"Calculating error\")\n",
    "    positive = (pred_floats) > 0.5\n",
    "    correct = np.equal(corelations, positive)\n",
    "    true_positive = land(correct, positive)\n",
    "    true_negative = land(correct, lnot(positive))\n",
    "    false_positive = land(lnot(correct), positive)\n",
    "    false_negative = land(lnot(correct), lnot(positive))\n",
    "\n",
    "    logger.debug(\"Summing errors by incidents\")\n",
    "    for tp, tn, fp, fn, i, j in zip(\n",
    "        true_positive, true_negative,\n",
    "        false_positive, false_negative,\n",
    "        iz, js\n",
    "    ):\n",
    "        error_dict[i] = error_dict.get(i, np.zeros(4)) + np.array([tp, tn, fp, fn])\n",
    "        error_dict[j] = error_dict.get(j, np.zeros(4)) + np.array([tp, tn, fp, fn])\n",
    "        \n",
    "(labels, errors) = zip(*sorted(list(error_dict.items())))\n",
    "errors = np.array(errors)\n",
    "errors_norm = errors / errors.sum(axis=1)[:, None]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "index = np.arange(len(labels))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "bar_width = 0.1\n",
    "\n",
    "\n",
    "\n",
    "types = ['True Positive', 'True Negative', 'False Positive', 'False Negative']\n",
    "colors = ['g', 'b', 'r', 'y']\n",
    "\n",
    "for i, (typ, color) in enumerate(zip(types, colors)):\n",
    "    rect = plt.bar(\n",
    "        index + bar_width*i,\n",
    "        errors_norm[:,i],\n",
    "        bar_width,\n",
    "        alpha=0.8,\n",
    "        color=color,\n",
    "        error_kw={'ecolor': '0.3'},\n",
    "        label=typ,\n",
    "    )\n",
    "\n",
    "plt.xlabel('Incidents')\n",
    "plt.ylabel('Rate')\n",
    "plt.xticks(index + bar_width, labels)\n",
    "plt.legend(bbox_to_anchor=(0., 1.02, 1., .102), loc=3,\n",
    "           ncol=2, mode=\"expand\", borderaxespad=0.)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(out_prefix+'detection_norm.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i, (typ, color) in enumerate(zip(types, colors)):\n",
    "    rect = plt.bar(\n",
    "        index + bar_width*i,\n",
    "        errors[:,i],\n",
    "        bar_width,\n",
    "        alpha=0.8,\n",
    "        color=color,\n",
    "        error_kw={'ecolor': '0.3'},\n",
    "        label=typ,\n",
    "    )\n",
    "\n",
    "plt.xlabel('Incidents')\n",
    "plt.ylabel('Rate')\n",
    "plt.xticks(index + bar_width, labels)\n",
    "plt.legend(bbox_to_anchor=(0., 1.02, 1., .102), loc=3,\n",
    "           ncol=2, mode=\"expand\", borderaxespad=0.)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(out_prefix+'detection_notnorm.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sys.exit(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = alert_to_vector(alerts1, masks1)\n",
    "y = iz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "def my_cluster_eval(y, y_pred, X, n_clusters):\n",
    "    print('Estimated number of clusters: %d' % n_clusters_)\n",
    "    print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(y, y_pred))\n",
    "    print(\"Completeness: %0.3f\" % metrics.completeness_score(y, y_pred))\n",
    "    print(\"V-measure: %0.3f\" % metrics.v_measure_score(y, y_pred))\n",
    "    print(\"Adjusted Rand Index: %0.3f\"\n",
    "          % metrics.adjusted_rand_score(y, y_pred))\n",
    "    print(\"Adjusted Mutual Information: %0.3f\"\n",
    "          % metrics.adjusted_mutual_info_score(y, y_pred))\n",
    "    print(\"Silhouette Coefficient: %0.3f\"\n",
    "          % metrics.silhouette_score(X, y_pred))\n",
    "    \n",
    "for eps in [0.01, 0.03, 0.1, 0.3, 1, 3, 10, 30]:\n",
    "    for min_samples in [1, 3, 10, 30]:\n",
    "        db = DBSCAN(eps=0.01, min_samples=3).fit(X)\n",
    "        core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "        core_samples_mask[db.core_sample_indices_] = True\n",
    "        y_pred = db.labels_\n",
    "        # Number of clusters in labels, ignoring noise if present.\n",
    "        n_clusters = len(set(y_pred)) - (1 if -1 in y_pred else 0)\n",
    "        \n",
    "        logger.info(\n",
    "            \"DBSCAN with eps={} and min_samples={} yielded {} clusters with a homogenity of {}\".format(\n",
    "                eps, min_samples, n_clusters, metrics.homogeneity_score(y, y_pred)\n",
    "            ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "my_cluster_eval(y, y_pred, X, n_clusters_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.unique(y_pred, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Assign label to clusters according which incident has the largest part of its alert in the given cluster\n",
    "# weight to handle class skew\n",
    "weights = {l: 1/cnt for (l, cnt) in zip(*np.unique(y, return_counts=True))}\n",
    "allocs = zip(y, y_pred)\n",
    "\n",
    "from collections import Counter\n",
    "c = Counter(map(tuple, allocs))\n",
    "\n",
    "mapper = dict()\n",
    "for _, (incident, cluster) in sorted([(c[k]*weights[k[0]], k) for k in c.keys()]):\n",
    "    mapper[cluster] = incident"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# misclassification matrix\n",
    "y_pred_inc = np.array([mapper[el] for el in y_pred])\n",
    "from sklearn import metrics\n",
    "cm = metrics.confusion_matrix(y, y_pred_inc)\n",
    "\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(((cm+1.0e-16) / cm.sum(axis=0))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Map any cluster to the incident that it is most often put in\n",
    "\n",
    "mapping = sorted(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mapper = dict()\n",
    "for _, m in mapping:\n",
    "    mapper[m[0]] = m[1]\n",
    "    \n",
    "y_pred = np.array([mapper[el] for el in y_pred])\n",
    "\n",
    "print(mapper.keys())\n",
    "print(mapper.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weights = {l: 1/cnt for (l, cnt) in zip(*np.unique(y, return_counts=True))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "zip(*np.unique(y, return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
