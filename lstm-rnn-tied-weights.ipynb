{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Copyright (C) Egon Kidmose 2015-2017\n",
    "\n",
    "This file is part of lstm-rnn-correlation.\n",
    "\n",
    "lstm-rnn-correlation is free software: you can redistribute it and/or\n",
    "modify it under the terms of the GNU Lesser General Public License as\n",
    "published by the Free Software Foundation, either version 3 of the\n",
    "License, or (at your option) any later version.\n",
    "\n",
    "lstm-rnn-correlation is distributed in the hope that it will be\n",
    "useful, but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n",
    "Lesser General Public License for more details.\n",
    "\n",
    "You should have received a copy of the GNU Lesser General Public\n",
    "License along with lstm-rnn-correlation. If not, see\n",
    "<http://www.gnu.org/licenses/>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alert correlation with a Long Short-Term Memoroy (LSTM) Recurrent Neural Network(RNN) and cosine similarity\n",
    "\n",
    "**Author:** Egon Kidmose, egk@es.aau.dk\n",
    "\n",
    "In network security a common task is to detect network intrusions and for this purpose an Intrusion Detections System (IDS) can be used to raise alerts on suspicious network traffic.\n",
    "Snort, Suricata and Bro are examples of free and open source IDSs (Commercial products also exist).\n",
    "The alerts generally provides low level information such as recognition of strings that are known to be part of security exploits or anomalous connection rates for a host.\n",
    "By grouping alerts that are correlated into higher level events, false positives might be suppressed and attack scenarios becomes easier to recognise.\n",
    "This is a take on how to correlate IDS alerts to determine which belong in the same group.\n",
    "\n",
    "Alerts can be represented as log lines with various information such as time stamp, IP adresses, protocol information and a description of what triggered the alert.\n",
    "It is assumed that such a log lines hold the information needed to determine if two alerts are correlated or not.\n",
    "\n",
    "The input to the neural network will be two alerts and the output will indicate if they are correlated or not.\n",
    "In further detail the inputs is two strings of ASCII characters of variable length.\n",
    "For the output a Cosine Similarity layer is implemented and used to produce an output in the range [-1,1], with -1 meaning opposite, 0 meaning orthogonal and 1 meaning the same.\n",
    "\n",
    "For the hidden layers only a single layers of Long Short-Term Memory (LSTM) cells is used.\n",
    "It is an option to experiment with adding more.\n",
    "Being reccurrent, such a layer handles variable length input well.\n",
    "\n",
    "While it turned out to be to challenging to implement, the initial idea was to let the two inputs pass through LSTM layers with identical weights.\n",
    "The intent was to have them act as transformations into a space where cosine similarity could be used to measure similarity of the alerts.\n",
    "However I have not succeded at tying the weights together.\n",
    "As an alternative this might be achieved by using all training pairs in both original and swapped order.\n",
    "The intuition is that this leads to two identical layers, but intuition also suggest that this is highly ineffective.\n",
    "\n",
    "                      Output\n",
    "                        |\n",
    "    Cosine similarity   #\n",
    "                       / \\\n",
    "        LSTM layers   #   #\n",
    "                      |   |\n",
    "        \"alert number 1\"  |\n",
    "            \"alert number 2\"\n",
    "\n",
    "\n",
    "Reference: Huang, Po-Sen, et al. \"Learning deep structured semantic models for web search using clickthrough data.\" Proceedings of the 22nd ACM international conference on Conference on information & knowledge management. ACM, 2013.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "import logging\n",
    "logging.getLogger().handlers = []\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import psutil\n",
    "import time\n",
    "import glob\n",
    "import json\n",
    "import subprocess\n",
    "import datetime\n",
    "import socket\n",
    "\n",
    "from operator import itemgetter\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib\n",
    "try: # If X is not available select backend not requiring X\n",
    "    os.environ['DISPLAY']\n",
    "except KeyError:\n",
    "    matplotlib.use('Agg')\n",
    "try: # If ipython, do inline\n",
    "    %matplotlib inline\n",
    "except NameError:\n",
    "    pass\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import lasagne\n",
    "from lasagne.layers import *\n",
    "from lasagne.nonlinearities import *\n",
    "from lasagne.updates import *\n",
    "from lasagne.objectives import *\n",
    "\n",
    "import lstm_rnn_tied_weights\n",
    "from lstm_rnn_tied_weights import CosineSimilarityLayer\n",
    "from lstm_rnn_tied_weights import load, modify, split, pool, cross_join, limit, break_down_data\n",
    "from lstm_rnn_tied_weights import iterate_minibatches, encode\n",
    "from lstm_rnn_tied_weights import mask_ips, mask_tss, mask_ports\n",
    "from lstm_rnn_tied_weights import uniquify_victim, extract_prio, get_discard_by_prio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = lstm_rnn_tied_weights.logger\n",
    "OUTPUT = 'output'\n",
    "runid = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "if os.environ.get('SLURM_ARRAY_TASK_ID', False):\n",
    "    runid += '-slurm-{}_{}'.format(\n",
    "        os.environ['SLURM_ARRAY_JOB_ID'],\n",
    "        os.environ['SLURM_ARRAY_TASK_ID']\n",
    "    )\n",
    "elif os.environ.get('SLURM_JOB_ID', False):\n",
    "    runid += '-slurm-' + os.environ['SLURM_JOB_ID']\n",
    "else:\n",
    "     runid += '-' + socket.gethostname()\n",
    "out_dir = OUTPUT + '/' + runid\n",
    "if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir)\n",
    "out_prefix = out_dir + '/' + runid + '-'\n",
    "# info log file\n",
    "infofh = logging.FileHandler(out_prefix + 'info.log')\n",
    "infofh.setLevel(logging.INFO)\n",
    "infofh.setFormatter(logging.Formatter(\n",
    "        fmt='%(message)s',\n",
    "))\n",
    "logger.addHandler(infofh)\n",
    "# verbose log file\n",
    "vfh = logging.FileHandler(out_prefix + 'verbose.log')\n",
    "vfh.setLevel(logging.DEBUG)\n",
    "vfh.setFormatter(logging.Formatter(\n",
    "        fmt='%(asctime)s - PID:%(process)d - %(levelname)s - %(message)s',\n",
    "))\n",
    "logger.addHandler(vfh)\n",
    "\n",
    "logger.info('Output, including logs, are going to: {}'.format(out_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = dict()\n",
    "\n",
    "# Data control\n",
    "env['CSVIN'] = os.environ.get('CSVIN', None)\n",
    "if env['CSVIN'] is None:\n",
    "    logger.critical(\"Cannot run without a CSV input\")\n",
    "    sys.exit(-1)\n",
    "env['MAX_PAIRS'] = int(os.environ.get('MAX_PAIRS', 0))\n",
    "env['BATCH_SIZE'] = int(os.environ.get('BATCH_SIZE', 10000))\n",
    "env['RAND_SEED'] = int(os.environ.get('RAND_SEED', time.time())) # Current unix time if not specified\n",
    "env['EPOCHS'] = int(os.environ.get('EPOCHS', 10))\n",
    "env['VAL_CUT'] = int(os.environ.get('VAL_CUT', -1))\n",
    "\n",
    "# Neural network\n",
    "env['NN_UNITS'] = [int(el) for el in os.environ.get('NN_UNITS', '10').split(',')]\n",
    "env['NN_LEARNING_RATE'] = float(os.environ.get('NN_LEARNING_RATE', '0.1'))\n",
    "\n",
    "# git\n",
    "env['version'] = subprocess.check_output([\"git\", \"describe\"]).strip()\n",
    "if not isinstance(env['version'], str):\n",
    "    env['version'] = str(env['version'], \"UTF-8\")\n",
    "\n",
    "# Platform\n",
    "env['OMP_NUM_THREADS'] = os.environ.get('OMP_NUM_THREADS', str())\n",
    "env['THEANO_FLAGS'] = os.environ.get('THEANO_FLAGS', str())\n",
    "env['MAX_HOURS'] = float(os.environ.get('MAX_HOURS', '23.5'))\n",
    "start_script = datetime.datetime.now()\n",
    "end_script_before = start_script + datetime.timedelta(hours=env['MAX_HOURS'])\n",
    "logger.info('Started at {}, must end before {}'.format(start_script, end_script_before))\n",
    "\n",
    "# Continue existing, old job\n",
    "env['OLD_JOB'] = os.environ.get('OLD_JOB', None)\n",
    "if env['OLD_JOB']:\n",
    "    logger.critical(\"Continuing on OLD_JOB={}\".format(env['OLD_JOB']))\n",
    "    if not os.path.exists(env['OLD_JOB']):\n",
    "        logger.critical(\"Old job to continue does not exist ({})\".format(env['OLD_JOB']))\n",
    "        sys.exit(-1)\n",
    "    old_run_id = env['OLD_JOB'].split('/')[-1]\n",
    "    old_out_prefix = env['OLD_JOB'] + '/' + old_run_id + '-'\n",
    "    with open(old_out_prefix + 'env.json') as f:\n",
    "        env_old = json.load(f)\n",
    "    logger.info('Loaded old env')\n",
    "    statics = [\n",
    "        'CSVIN',\n",
    "        'MAX_PAIRS',\n",
    "        'BATCH_SIZE',\n",
    "        'NN_UNITS',\n",
    "        'NN_LEARNING_RATE',\n",
    "        'VAL_CUT',\n",
    "    ]\n",
    "    for s in statics:\n",
    "        env[s] = env_old[s]\n",
    "\n",
    "assert (0 <= env['VAL_CUT']) and (env['VAL_CUT'] <= 9), \"Invalid cross validation cut: {}\".format(env['VAL_CUT'])\n",
    "\n",
    "logger.info(\"Starting.\")\n",
    "logger.info(\"env: \" + str(env))\n",
    "for k in sorted(env.keys()):\n",
    "    logger.info('env[\\'{}\\']: {}'.format(k,env[k]))\n",
    "\n",
    "logger.debug('Saving env')\n",
    "with open(out_prefix + 'env.json', 'w') as f:\n",
    "    json.dump(env, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = env['RAND_SEED']\n",
    "def rndseed():\n",
    "    global seed\n",
    "    seed += 1\n",
    "    return seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Timer(object):\n",
    "    def __init__(self, name='', log=logger.debug):\n",
    "        self.name = name\n",
    "        self.log = log\n",
    "        \n",
    "    def __enter__(self):\n",
    "        self.log('Timer(%s) started' % (self.name, ))\n",
    "        self.start = time.time()\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, *args):\n",
    "        self.end = time.time()\n",
    "        self.dur = datetime.timedelta(seconds=self.end - self.start)\n",
    "        self.log('Timer(%s):\\t%s' % (self.name, self.dur))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data for unit testing\n",
    "X_unit = ['abcdef', 'abcdef', 'qwerty']\n",
    "X_unit = [[ord(c) for c in w] for w in X_unit]\n",
    "X_unit = np.array(X_unit, dtype='int32')\n",
    "logger.debug(X_unit)\n",
    "n_alerts_unit, l_alerts_unit = X_unit.shape\n",
    "mask_unit = np.ones(X_unit.shape, dtype=theano.config.floatX)\n",
    "logger.debug(mask_unit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensions\n",
    "n_alerts = None\n",
    "l_alerts = None\n",
    "n_alphabet = 2**7 # All ASCII chars\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Symbolic variables\n",
    "input_var = T.imatrix('inputs')\n",
    "input_var2 = T.imatrix('inputs2')\n",
    "mask_var = T.matrix('masks')\n",
    "mask_var2 = T.matrix('masks2')\n",
    "target_var = T.vector('targets')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_in = InputLayer(shape=(n_alerts, l_alerts), input_var=input_var, name='INPUT-LAYER')\n",
    "\n",
    "l_in_output_var = get_output(l_in, inputs={l_in: input_var})\n",
    "assert l_in_output_var.dtype == 'int32'\n",
    "\n",
    "pred_unit = l_in_output_var.eval({input_var: X_unit})\n",
    "assert (pred_unit == X_unit).all(), \"Unexpected output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_emb = EmbeddingLayer(\n",
    "    l_in, n_alphabet, n_alphabet,\n",
    "    W=np.eye(n_alphabet, dtype=theano.config.floatX),\n",
    "    name='EMBEDDING-LAYER',\n",
    ")\n",
    "l_emb.params[l_emb.W].remove('trainable') # Fix weight\n",
    "\n",
    "l_emb_output_var = get_output(l_emb, inputs={l_in: input_var})\n",
    "assert l_emb_output_var.dtype == theano.config.floatX\n",
    "\n",
    "pred_unit = l_emb_output_var.eval({input_var: X_unit})\n",
    "assert (np.argmax(pred_unit, axis=2) == X_unit).all()\n",
    "assert np.all(pred_unit.shape == (n_alerts_unit, l_alerts_unit, n_alphabet ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_mask = InputLayer(shape=(n_alerts, l_alerts), input_var=mask_var, name='MASK-INPUT-LAYER')\n",
    "\n",
    "l_mask_output_var = get_output(l_mask, inputs={l_mask: mask_var})\n",
    "assert l_mask_output_var.dtype == theano.config.floatX\n",
    "\n",
    "pred_unit = l_mask_output_var.eval({mask_var: mask_unit})\n",
    "assert (pred_unit == mask_unit).all(), \"Unexpected output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_lstm = l_emb\n",
    "for i, num_units in enumerate(env['NN_UNITS']):\n",
    "    logger.info('Adding {} units for {} layer'.format(num_units, i))\n",
    "    l_lstm = LSTMLayer(l_lstm, num_units=num_units, name='LSTM-LAYER[{}]'.format(i), mask_input=l_mask)\n",
    "\n",
    "l_lstm_output_var = get_output(l_lstm, inputs={l_in: input_var, l_mask: mask_var})\n",
    "assert l_mask_output_var.dtype == theano.config.floatX\n",
    "\n",
    "pred_unit = l_lstm_output_var.eval({input_var: X_unit, mask_var: mask_unit})\n",
    "assert pred_unit.dtype == theano.config.floatX, \"Unexpected dtype\"\n",
    "assert pred_unit.shape == (n_alerts_unit, l_alerts_unit, num_units), \"Unexpected dimensions\"\n",
    "pred_unit = l_lstm_output_var.eval({input_var: [[1],[1]], mask_var: [[1],[1]]})\n",
    "assert np.all(pred_unit[0] == pred_unit[1]), \"Repeated alerts must produce the same\"\n",
    "pred_unit = l_lstm_output_var.eval({input_var: [[1,1],[1,1]], mask_var: [[1,1],[1,1]]})\n",
    "assert np.all(pred_unit[0] == pred_unit[1]), \"Repeated alerts must produce the same\"\n",
    "pred_unit = l_lstm_output_var.eval({input_var: [[1,1],[0,1]], mask_var: [[1,1],[1,1]]})\n",
    "assert np.all(pred_unit[0] != pred_unit[1]), \"Earlier must affect laters\"\n",
    "pred_unit = l_lstm_output_var.eval({input_var: [[1,0],[1,1]], mask_var: [[1,1],[1,1]]})\n",
    "assert np.all(pred_unit[0,0] == pred_unit[1,0]), \"Later must not affect earlier\"\n",
    "assert np.all(pred_unit[0,1] != pred_unit[1,1]), \"Current must make a difference\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_slice = SliceLayer(l_lstm, indices=-1, axis=1, name=\"SLICE-LAYER\") # Only last timestep\n",
    "\n",
    "l_slice_output_var = get_output(l_slice, inputs={l_in: input_var, l_mask: mask_var})\n",
    "assert l_slice_output_var.dtype == theano.config.floatX\n",
    "\n",
    "pred_unit = l_slice_output_var.eval({input_var: X_unit, mask_var: mask_unit})\n",
    "assert pred_unit.shape == (n_alerts_unit, num_units), \"Unexpected shape\"\n",
    "pred_unit_lstm = l_lstm_output_var.eval({input_var: X_unit, mask_var: mask_unit})\n",
    "assert np.all(pred_unit_lstm[:, -1, :] == pred_unit), \"Unexpected result of slicing\"\n",
    "\n",
    "net = l_slice\n",
    "logger.info('First line built')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second line as a copy with shared weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_in2 = InputLayer(shape=l_in.shape, input_var=input_var2, name=l_in.name+'2')\n",
    "l_mask2 = InputLayer(shape=l_mask.shape, input_var=mask_var2, name=l_mask.name+'2')\n",
    "net2 = lstm_rnn_tied_weights.clone(net, l_in2, l_mask2)\n",
    "\n",
    "net_pred = get_output(net, inputs={l_in: input_var, l_mask: mask_var}).eval({input_var: X_unit, mask_var: mask_unit})\n",
    "net_pred2 = get_output(net2, inputs={l_in2: input_var, l_mask2: mask_var}).eval({input_var: X_unit, mask_var: mask_unit})\n",
    "assert (net_pred == net_pred2).all(), \"Output mismatch, two lines must produce same output\"\n",
    "\n",
    "logger.info('Second line built')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_cos = CosineSimilarityLayer(net, net2, name=\"COSINE-SIMILARITY-LAYER\")\n",
    "\n",
    "l_cos_output_var = get_output(l_cos, inputs={\n",
    "        l_in: input_var,\n",
    "        l_mask: mask_var,\n",
    "        l_in2: input_var2,\n",
    "        l_mask2: mask_var2,\n",
    "})\n",
    "assert l_emb_output_var.dtype == theano.config.floatX\n",
    "\n",
    "pred_unit = l_cos_output_var.eval(({\n",
    "            input_var: X_unit,\n",
    "            input_var2: X_unit,\n",
    "            mask_var: mask_unit,\n",
    "            mask_var2: mask_unit,\n",
    "}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_sig = NonlinearityLayer(l_cos, nonlinearity=sigmoid, name=\"SIGMOID-LAYER\")\n",
    "\n",
    "l_sig_output_var = get_output(l_sig, inputs={\n",
    "        l_in: input_var,\n",
    "        l_mask: mask_var,\n",
    "        l_in2: input_var2,\n",
    "        l_mask2: mask_var2,\n",
    "})\n",
    "assert l_sig_output_var.dtype == theano.config.floatX\n",
    "\n",
    "pred_unit = l_sig_output_var.eval(({\n",
    "            input_var: X_unit,\n",
    "            input_var2: X_unit,\n",
    "            mask_var: mask_unit,\n",
    "            mask_var2: mask_unit,\n",
    "}))\n",
    "assert pred_unit is not None\n",
    "\n",
    "cos_net = l_sig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Timer('Compiling theano', logger.info):\n",
    "    # Training Procedure\n",
    "    prediction = get_output(cos_net)\n",
    "    loss = binary_crossentropy(prediction, target_var)\n",
    "    loss = loss.mean()\n",
    "    params = get_all_params(cos_net, trainable=True)\n",
    "    updates = sgd(loss, params, learning_rate=env['NN_LEARNING_RATE'])\n",
    "\n",
    "    # Testing Procedure\n",
    "    test_prediction = get_output(cos_net, deterministic=True)\n",
    "    test_loss = binary_crossentropy(test_prediction, target_var)\n",
    "    test_loss = test_loss.mean()\n",
    "    test_acc = T.mean(T.eq(test_prediction > 0.5, target_var),\n",
    "                      dtype=theano.config.floatX)\n",
    "\n",
    "    train_fn = theano.function([input_var, input_var2, mask_var, mask_var2, target_var], loss, updates=updates)\n",
    "    val_fn = theano.function([input_var, input_var2, mask_var, mask_var2, target_var], [test_loss, test_acc])\n",
    "    prediction_fn = theano.function([input_var, input_var2, mask_var, mask_var2], prediction)\n",
    "\n",
    "    alert_to_vector = theano.function([input_var, mask_var], get_output(l_slice))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Timer('Load data'):\n",
    "    data = pd.read_csv(env['CSVIN'])\n",
    "\n",
    "# Test data\n",
    "test_incidents = np.array(['1', '2', 'benign']*2)\n",
    "test_alerts = np.array(\n",
    "    [\n",
    "        'alert %s of incident %s' % (a, i)\n",
    "         for i, a in zip(test_incidents, np.arange(len(test_incidents)))\n",
    "    ]\n",
    ")\n",
    "test_data1 = pd.DataFrame(\n",
    "    np.concatenate([test_incidents.reshape(6, 1), test_alerts.reshape(6, 1)], axis=1),\n",
    "    columns=['incident', 'alert']\n",
    ")\n",
    "test_data1['cut'] = 0\n",
    "test_data2 = test_data1.copy()[:2]\n",
    "test_data2['cut'] = 1\n",
    "test_data = pd.concat([test_data1, test_data2]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = data['alert'].apply(len).max()\n",
    "\n",
    "def encode_alert(alert):\n",
    "    encoded_alert = np.zeros(max_len, dtype='int8')\n",
    "    encoded_alert[:len(alert)] = map(ord, alert)\n",
    "    return encoded_alert\n",
    "\n",
    "def build_mask(encoded_alert):\n",
    "    mask = (encoded_alert > 0).astype('int8')\n",
    "    return mask\n",
    "\n",
    "# Test\n",
    "test_alert = data['alert'].iloc[0]\n",
    "test_encoded_alert = encode_alert(test_alert)\n",
    "test_mask = build_mask(test_encoded_alert)\n",
    "assert ''.join(map(chr,test_encoded_alert[test_mask.astype(bool)])) == test_alert,\\\n",
    "    \"First alert cannot be encoded and decoded: %s\" % test_alert\n",
    "assert data[data.incident != 'benign'].incident.map(int).all(), \"Invalid incident IDs - must be int\"\n",
    "\n",
    "with Timer('Encode alerts'):\n",
    "    data['encoded_alert'] = data['alert'].apply(encode_alert)\n",
    "    data['mask'] = data['encoded_alert'].apply(build_mask)\n",
    "\n",
    "    # incident as int, -1 represent benign\n",
    "    data['incident'] = pd.to_numeric(data['incident'], errors='coerce').fillna(-1).astype(int)\n",
    "\n",
    "assert (data['alert'].map(len) == data['mask'].map(sum)).all(), \"Sum of mask is not equal length of alert\"\n",
    "assert (data['mask'].map(np.nonzero).map(np.max)+1 == data['alert'].map(len)).all(), \\\n",
    "    \"Last non-zero index of mask is not equal to length of alert\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# test mapping speed\n",
    "\n",
    "def alert_mask_iter(data, batch_size):\n",
    "    ii = 0 # minibatch counter\n",
    "    assert len(data) >= batch_size, \\\n",
    "        \"{} samples is not enough to produce a minibatch of {} samples\"\\\n",
    "        .format(len(data), batch_size)\n",
    "    logger.debug('Expect %d minibatches' % (len(data)//batch_size))\n",
    "    while len(data) - ii * batch_size >= batch_size:\n",
    "        with Timer('Minibatch{}'.format(ii)):\n",
    "            begin = ii * batch_size\n",
    "            ii += 1\n",
    "            end = ii * batch_size\n",
    "            logger.debug(\"Producing minibatch no. %d\" % ii)\n",
    "            batch = data.iloc[begin:end]\n",
    "            inputs = np.vstack(batch['encoded_alert'].as_matrix())\n",
    "            mask = np.vstack(batch['mask'].as_matrix())\n",
    "            yield inputs, mask\n",
    "\n",
    "for mbatch_size in [100, 300, 1000, 3000, 4316]:\n",
    "    if mbatch_size < data.shape[0]:\n",
    "        logger.warn(\n",
    "            \"Not testing mapping speed for batch size=%d (data only holds %d samples).\" % \\\n",
    "            (mbatch_size, data.shape[0])\n",
    "        )\n",
    "    for mbatch_alerts, mbatch_masks in alert_mask_iter(data, mbatch_size):\n",
    "        start_mbatch = datetime.datetime.now()\n",
    "        _ = alert_to_vector(mbatch_alerts, mbatch_masks)\n",
    "        speed = mbatch_alerts.shape[0]/(datetime.datetime.now()-start_mbatch).total_seconds()\n",
    "        logger.info('Minibatch (Mapping, mbatch_size={}) completed. speed={:.3f} [alerts/sec]'.format(mbatch_size, speed))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cut data, pairing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pairs(data):\n",
    "    KEY = 'DUMMY_MERGE_KEY'\n",
    "    assert KEY not in data.columns\n",
    "    data = data.copy(deep=False)\n",
    "    data[KEY] = 0\n",
    "    pairs = pd.merge(data, data, on=KEY)\n",
    "    pairs.drop(KEY , axis=1, inplace=True)\n",
    "    return pairs\n",
    "\n",
    "# Test get_pairs\n",
    "test_pairs = get_pairs(test_data)\n",
    "assert len(test_pairs) == len(test_data) ** 2\n",
    "\n",
    "def is_correlated(row):\n",
    "    \"\"\"\n",
    "    row[0], row[1] : ints for incidents, with benign encoded as -1\n",
    "    \"\"\"\n",
    "    return (row[0]!=-1) & (row[1]!=-1) & (row[0]==row[1])\n",
    "\n",
    "assert is_correlated([0, 0]), \"Same incident must result in true\"\n",
    "assert not is_correlated([0, 1]), \"Different incident must result in false\"\n",
    "assert not is_correlated([0, -1]), \"Benign must result in false\"\n",
    "assert not is_correlated([-1, -1]), \"Benign must result in false\"\n",
    "\n",
    "def add_cor_col(pairs):\n",
    "    pairs = pairs.copy(deep=False)\n",
    "    pairs['cor'] = pairs[['incident_x', 'incident_y']].apply(is_correlated, raw=True, axis=1)\n",
    "    return pairs\n",
    "    \n",
    "def shuffle(pairs):\n",
    "    np.random.seed(rndseed())\n",
    "    return pairs.reindex(np.random.permutation(pairs.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alerts_train = data[data.cut != env['VAL_CUT']]\n",
    "alerts_val = data[data.cut == env['VAL_CUT']]\n",
    "\n",
    "with Timer('Build pairs'):\n",
    "    pairs_train = add_cor_col(shuffle(get_pairs(alerts_train)))\n",
    "    pairs_val = add_cor_col(shuffle(get_pairs(alerts_val)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_minibatches(pairs, batch_size, max_pairs=0, include_incidents=False):\n",
    "    ii = 0 # minibatch counter\n",
    "    if max_pairs != 0:\n",
    "        logger.debug('Limiting to {}'.format(max_pairs))\n",
    "        pairs = pairs.head(max_pairs)\n",
    "    assert len(pairs) >= batch_size, \\\n",
    "        \"{} samples is not enough to produce a minibatch of {} samples\"\\\n",
    "        .format(len(pairs), batch_size)\n",
    "    logger.info('Expect %d minibatches' % (len(pairs)//batch_size))\n",
    "    while len(pairs) - ii * batch_size >= batch_size:\n",
    "        with Timer('Minibatch{}'.format(ii)):\n",
    "            begin = ii * batch_size\n",
    "            ii += 1\n",
    "            end = ii * batch_size\n",
    "            logger.debug(\"Producing minibatch no. %d\" % ii)\n",
    "            batch = pairs.iloc[begin:end]\n",
    "            inputs1 = np.array(batch['encoded_alert_x'].values.tolist())\n",
    "            inputs2 = np.array(batch['encoded_alert_y'].values.tolist())\n",
    "            masks1 = np.array(batch['mask_x'].values.tolist())\n",
    "            masks2 = np.array(batch['mask_y'].values.tolist())\n",
    "            targets = np.array(batch['cor'].values.tolist())\n",
    "            if include_incidents:\n",
    "                incidents1 = np.array(batch['incident_x'].values.tolist())\n",
    "                incidents2 = np.array(batch['incident_y'].values.tolist())\n",
    "                yield inputs1, inputs2, masks1, masks2, targets, incidents1, incidents2\n",
    "            else:\n",
    "                 yield inputs1, inputs2, masks1, masks2, targets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Empirical Distribution Functions for model output, by ground truth for correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs_train_edf_cor = pairs_train[pairs_train['cor']==True].head(1000)\n",
    "pairs_train_edf_uncor = pairs_train[pairs_train['cor']==False].head(1000)\n",
    "pairs_val_edf_cor = pairs_val[pairs_val['cor']==True].head(1000)\n",
    "pairs_val_edf_uncor = pairs_val[pairs_val['cor']==False].head(1000)\n",
    "\n",
    "_, edf_bins = np.histogram([], bins=100, range=(0,1))\n",
    "edf_bin_centers = np.vstack((edf_bins[:-1].T, edf_bins[1:].T)).mean(axis=0)\n",
    "\n",
    "def get_hist(pairs):\n",
    "    inputs1, inputs2, masks1, masks2, targets = next(\n",
    "        iterate_minibatches(pairs, batch_size=len(pairs)),\n",
    "    )\n",
    "    hist, _ = np.histogram(\n",
    "        prediction_fn(inputs1, inputs2, masks1, masks2),\n",
    "        bins=edf_bins,\n",
    "    )\n",
    "    return hist\n",
    "\n",
    "def get_hists():\n",
    "    return {\n",
    "        'training' : {\n",
    "            'correlated' : get_hist(pairs_train_edf_cor),\n",
    "            'uncorrelated' : get_hist(pairs_train_edf_uncor),\n",
    "        },\n",
    "        'validation' : {\n",
    "            'correlated' : get_hist(pairs_val_edf_cor),\n",
    "            'uncorrelated' : get_hist(pairs_val_edf_uncor),\n",
    "        },\n",
    "    }\n",
    "\n",
    "def plot_hists(hists):\n",
    "    epochs = sorted(hists.keys())\n",
    "    \n",
    "    for sett in {'training', 'validation'}:\n",
    "        for epoch in epochs:\n",
    "            plt.figure()\n",
    "            for c in {'correlated', 'uncorrelated'}:\n",
    "                plt.plot(\n",
    "                    edf_bin_centers,\n",
    "                    hists[epoch][sett][c],\n",
    "                    label=c,\n",
    "                )\n",
    "            plt.title(\n",
    "                'EDF on %s data, trained for %d epochs' % (sett, epoch),\n",
    "            )\n",
    "            plt.legend()\n",
    "            max_val = 0\n",
    "            for v1 in hists.values():\n",
    "                for v2 in v1.values():\n",
    "                    for v3 in v2.values():\n",
    "                        max_val = max(v3.max(), max_val)\n",
    "            plt.gca().set_ylim(0, max_val)\n",
    "            plt.savefig(\n",
    "                out_prefix + 'edf_%s_%.2d.pdf' % (sett, epoch),\n",
    "                bbox_inches='tight',\n",
    "            )\n",
    "            plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perf_eval():\n",
    "    logger.debug('Starting performance evaluation on training data')\n",
    "    train_err = 0\n",
    "    train_acc = 0\n",
    "    train_mbatches = 0\n",
    "    for mbatch in iterate_minibatches(pairs_train, env['BATCH_SIZE'], env['MAX_PAIRS']):\n",
    "        err, acc = val_fn(*mbatch)\n",
    "        train_err += err\n",
    "        train_acc += acc\n",
    "        train_mbatches += 1\n",
    "    train_err = train_err/train_mbatches\n",
    "    train_acc = train_acc/train_mbatches\n",
    "    logger.debug(\n",
    "        'Completed performance evaluation on training data, err={}, acc={}'.format(\n",
    "            train_err, train_acc,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    logger.debug('Starting performance evaluation on validation data')\n",
    "    val_err = 0\n",
    "    val_acc = 0\n",
    "    val_mbatches = 0\n",
    "    for mbatch in iterate_minibatches(pairs_val, env['BATCH_SIZE'], env['MAX_PAIRS']):\n",
    "        err, acc = val_fn(*mbatch)\n",
    "        val_err += err\n",
    "        val_acc += acc\n",
    "        val_mbatches += 1\n",
    "    val_err = val_err/val_mbatches\n",
    "    val_acc = val_acc/val_mbatches\n",
    "    logger.debug(\n",
    "        'Completed performance evaluation on validation data, err={}, acc={}'.format(\n",
    "            val_err, val_acc,\n",
    "        )\n",
    "    )\n",
    "    return {\n",
    "        'training':{\n",
    "            'error': train_err,\n",
    "            'accuracy': train_acc,\n",
    "        },\n",
    "        'validation':{\n",
    "            'error': val_err,\n",
    "            'accuracy': val_acc,\n",
    "        }\n",
    "    }\n",
    "\n",
    "def plot_perfs(perfs):\n",
    "    for metric in ['accuracy', 'error']:\n",
    "        plt.figure()\n",
    "        for zet in ['training', 'validation']:\n",
    "            y = [perf[zet][metric] for epoch, perf in sorted(perfs.items())]\n",
    "            x = range(len(y))\n",
    "            plt.plot(x, y, '.-', label='{}'.format(zet))\n",
    "        plt.title('Performance ({}) over epochs'.format(metric))\n",
    "        plt.legend()\n",
    "        plt.xticks(perfs.keys())\n",
    "        plt.savefig(\n",
    "            out_prefix + 'perf_{}.pdf'.format(metric),\n",
    "            bbox_inches='tight',\n",
    "        )\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(net, filename):\n",
    "    logger.info('Loading model from {}'.format(filename))\n",
    "    with open(filename) as f:\n",
    "        model = json.loads(f.read())\n",
    "\n",
    "    # Order according to current model (JSON might reorder)\n",
    "    params = get_all_params(cos_net)\n",
    "    values = [np.array(model['model'][p.name], dtype=theano.config.floatX) for p in params]\n",
    "\n",
    "    set_all_param_values(cos_net, values)\n",
    "    logger.info(\"Model loaded\")\n",
    "\n",
    "def dump_model(net, filename):\n",
    "    model = {'model':{str(p): v.tolist() for p, v in zip(get_all_params(net), get_all_param_values(net))}}\n",
    "    logger.info('Dumping model to {}'.format(filename))\n",
    "    with open(filename, 'w') as f:\n",
    "        f.write(json.dumps(model))\n",
    "    logger.info('Model dumped')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load old job for continuation or start new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick'n'dirty numpy<->json encoding/decoding\n",
    "def json_encode_hists(d):\n",
    "    return {k:{k:{k:v.tolist() for k,v in v.items()} for k,v in v.items()} for k, v in d.items()}\n",
    "def json_decode_hists(d):\n",
    "    return {int(k):{str(k):{str(k):np.array(v) for k,v in v.items()} for k,v in v.items()} for k, v in d.items()}\n",
    "def json_encode_perfs(d):\n",
    "    return d\n",
    "def json_decode_perfs(d):\n",
    "    return {int(k):{str(k):{str(k):v for k,v in v.items()} for k,v in v.items()} for k, v in d.items()}\n",
    "\n",
    "if env['OLD_JOB']:\n",
    "    logger.info(\"Loading data for OLD_JOB: {}\".format(env['OLD_JOB']))\n",
    "    \n",
    "    logger.debug('Loading histograms')\n",
    "    with open(old_out_prefix + 'histograms.json') as f:\n",
    "        hists = json_decode_hists(json.load(f))\n",
    "\n",
    "    logger.debug('Loading perfomance metrics')\n",
    "    with open(old_out_prefix + 'performances.json') as f:\n",
    "        perfs = json_decode_perfs(json.load(f))\n",
    "    \n",
    "    assert sorted(hists.keys()) == sorted(perfs.keys())\n",
    "    completed_epochs = max(hists.keys())\n",
    "    \n",
    "    load_model(\n",
    "        cos_net,\n",
    "        (old_out_prefix + 'model{:04d}.json').format(completed_epochs),\n",
    "    )\n",
    "        \n",
    "else:\n",
    "    logger.info('Starting job from scratch (No OLD_JOB given)')\n",
    "    hists = {0: get_hists()}\n",
    "    perfs = {0: perf_eval()}\n",
    "    # Model already randomly initialised\n",
    "    completed_epochs = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info('Pre-training evaluation (on random model weights/loaded model as per above)')\n",
    "logger.debug(\"Performance evaluation before training: {}\".format(json.dumps(perfs[completed_epochs])))\n",
    "logger.info(\"  training error:\\t\\t{:.20f}\".format(perfs[completed_epochs]['training']['error']))\n",
    "logger.info(\"  training accuracy:\\t\\t{:.2f} %\".format(perfs[completed_epochs]['training']['accuracy'] * 100))\n",
    "logger.info(\"  validation error:\\t\\t{:.20f}\".format(perfs[completed_epochs]['validation']['error']))\n",
    "logger.info(\"  validation accuracy:\\t\\t{:.2f} %\".format(perfs[completed_epochs]['validation']['accuracy'] * 100))\n",
    "\n",
    "logger.info(\"Training for {} epochs on top of {} old epochs\".format(env['EPOCHS'], completed_epochs))\n",
    "for epoch, completed_epochs in enumerate(range(\n",
    "        completed_epochs+1, \n",
    "        completed_epochs+env['EPOCHS']+1\n",
    "    )):\n",
    "    train_mbatches = 0\n",
    "    start_epoch = datetime.datetime.now()\n",
    "    with Timer('Shuffle, epoch {}'.format(epoch)):\n",
    "        pairs_train = shuffle(pairs_train)\n",
    "    for mbatch in iterate_minibatches(pairs_train, env['BATCH_SIZE'], env['MAX_PAIRS']):\n",
    "        start_mbatch = datetime.datetime.now()\n",
    "        with Timer('Train epoch {}'.format(epoch)):\n",
    "            train_fn(*mbatch)\n",
    "        train_mbatches += 1\n",
    "        n_pairs_mbatch = mbatch[0].shape[0]\n",
    "        speed = n_pairs_mbatch/(datetime.datetime.now()-start_mbatch).total_seconds()\n",
    "        logger.debug('Minibatch completed. speed=%d [pairs/sec]' % speed)\n",
    "\n",
    "    with Timer('Validation epoch {}'.format(epoch)):\n",
    "        perfs[completed_epochs] = perf_eval()\n",
    "    logger.debug(\"Performance evaluation after epoch {}({} total): {}\".format(\n",
    "            epoch, completed_epochs, json.dumps(perfs[completed_epochs])))\n",
    "    logger.info(\"  training error:\\t\\t{:.20f}\".format(perfs[completed_epochs]['training']['error']))\n",
    "    logger.info(\"  training accuracy:\\t\\t{:.2f} %\".format(perfs[completed_epochs]['training']['accuracy'] * 100))\n",
    "    logger.info(\"  validation error:\\t\\t{:.20f}\".format(perfs[completed_epochs]['validation']['error']))\n",
    "    logger.info(\"  validation accuracy:\\t\\t{:.2f} %\".format(perfs[completed_epochs]['validation']['accuracy'] * 100))\n",
    "\n",
    "    hists[completed_epochs] = get_hists()\n",
    "    dump_model(cos_net, out_prefix + 'model' + str(completed_epochs).zfill(4)+ '.json')\n",
    "\n",
    "    end_epoch = datetime.datetime.now()\n",
    "    dur_epoch = end_epoch-start_epoch\n",
    "    logger.info(\"Completed epoch %s of %d, time=%.3f[sec]\" \\\n",
    "                % (epoch + 1, env['EPOCHS'], dur_epoch.total_seconds()))\n",
    "    logger.info('Timer(Epoch)\\t\\t%s' % dur_epoch)\n",
    "    \n",
    "    if datetime.datetime.now() + dur_epoch > end_script_before:\n",
    "        logger.warning(\"Skipping any remaining epochs as last epoch took longer than remaining time\")\n",
    "        break\n",
    "\n",
    "logger.info('Training complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info('Dumping histograms')\n",
    "with open(out_prefix + 'histograms.json', 'w') as f:\n",
    "    json.dump(json_encode_hists(hists), f)\n",
    "    \n",
    "logger.info('Dumping perfomance metrics')\n",
    "with open(out_prefix + 'performances.json', 'w') as f:\n",
    "    json.dump(json_encode_perfs(perfs), f)\n",
    "\n",
    "logger.info('Plotting histograms')\n",
    "plot_hists(hists)\n",
    "\n",
    "logger.info('Plotting performance')\n",
    "plot_perfs(perfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse errors in correlation detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count errors\n",
    "error_dict = dict()\n",
    "\n",
    "land = np.logical_and\n",
    "lnot = np.logical_not\n",
    "\n",
    "for mbatch in iterate_minibatches(pairs_val, env['BATCH_SIZE'], env['MAX_PAIRS'], include_incidents=True):\n",
    "    alerts1, alerts2, masks1, masks2, corelations, incidents1, incidents2 = mbatch\n",
    "    pred_floats = prediction_fn(alerts1, alerts2, masks1, masks2)\n",
    "\n",
    "    logger.debug(\"Calculating error\")\n",
    "    positive = (pred_floats) > 0.5\n",
    "    correct = np.equal(corelations, positive)\n",
    "    true_positive = land(correct, positive)\n",
    "    true_negative = land(correct, lnot(positive))\n",
    "    false_positive = land(lnot(correct), positive)\n",
    "    false_negative = land(lnot(correct), lnot(positive))\n",
    "\n",
    "    logger.debug(\"Summing errors by incidents\")\n",
    "    for tp, tn, fp, fn, i, j in zip(\n",
    "        true_positive, true_negative,\n",
    "        false_positive, false_negative,\n",
    "        incidents1, incidents2\n",
    "    ):\n",
    "        error_dict[i] = error_dict.get(i, np.zeros(4)) + np.array([tp, tn, fp, fn])\n",
    "        error_dict[j] = error_dict.get(j, np.zeros(4)) + np.array([tp, tn, fp, fn])\n",
    "(labels, errors) = zip(*sorted(list(error_dict.items())))\n",
    "# Proper label for benign\n",
    "assert labels[0] == -1, \"Expecting first label to be benign, encoded by -1\"\n",
    "labels = tuple(['benign']) + labels[1:]\n",
    "\n",
    "cols = ['TP', 'TN', 'FP', 'FN']\n",
    "errors = pd.DataFrame(np.array(errors), columns=cols, index=labels, dtype=int)\n",
    "errors.loc['total'] = errors.sum()\n",
    "\n",
    "# Normalised errors\n",
    "cols_norm = [c + ' (Norm.)' for c in cols]\n",
    "errors[cols_norm] = (errors[cols].T / errors[cols].sum(axis=1)).T\n",
    "\n",
    "# error rates\n",
    "cols_rate = ['TPR', 'TNR', 'FPR', 'FNR']\n",
    "errors['TPR'] = errors['TP'].astype(float) / (errors['TP'] + errors['FN'])\n",
    "errors['TNR'] = errors['TN'].astype(float) / (errors['TN'] + errors['FP'])\n",
    "errors['FPR'] = errors['FP'].astype(float) / (errors['TN'] + errors['FP'])\n",
    "errors['FNR'] = errors['FN'].astype(float) / (errors['TP'] + errors['FN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.debug('Errors table for latex: ' + errors[cols].to_latex())\n",
    "logger.info('Errors table:\\n'+ errors[cols].to_string())\n",
    "errors[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.debug('Normalised errors table for latex: ' + errors[cols_norm].to_latex())\n",
    "logger.info('Normalised errors table:\\n'+ errors[cols_norm].to_string())\n",
    "errors[cols_norm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.debug('Normalised errors table for latex: ' + errors[cols_norm].to_latex())\n",
    "logger.info('Normalised errors table:\\n'+ errors[cols_norm].to_string())\n",
    "errors[cols_norm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants for plotting\n",
    "index_x = np.arange(len(labels))\n",
    "bar_width = 0.2\n",
    "colors = ['g', 'b', 'r', 'y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "for x, (metric, color) in enumerate(zip(cols_norm, colors)):\n",
    "    y = errors[metric].iloc[:-1] # skip total\n",
    "    rect = plt.bar(\n",
    "        index_x + bar_width * x,\n",
    "        y,\n",
    "        bar_width,\n",
    "        alpha=0.8,\n",
    "        color=color,\n",
    "        error_kw={'ecolor': '0.3'},\n",
    "        label=metric,\n",
    "    )\n",
    "plt.title('Detection outcomes pr. incident (Normalised)')\n",
    "plt.xlabel('Incident')\n",
    "plt.ylabel('Rate')\n",
    "plt.ylim(0, 1)\n",
    "plt.xticks(index_x + bar_width, labels)\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.tight_layout()\n",
    "plt.savefig(out_prefix+'detection_norm.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "for x, (metric, color) in enumerate(zip(cols, colors)):\n",
    "    y = errors[metric].iloc[:-1] # skip total\n",
    "    rect = plt.bar(\n",
    "        index_x + bar_width * x,\n",
    "        y,\n",
    "        bar_width,\n",
    "        alpha=0.8,\n",
    "        color=color,\n",
    "        error_kw={'ecolor': '0.3'},\n",
    "        label=metric,\n",
    "    )\n",
    "plt.title('Detection outcomes pr. incident')\n",
    "plt.xlabel('Incident')\n",
    "plt.ylabel('Count (Pairs)')\n",
    "plt.xticks(index_x + bar_width, labels)\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.tight_layout()\n",
    "plt.savefig(out_prefix+'detection_notnorm.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info('Complete error table:\\n'+errors.to_string())\n",
    "logger.debug('Complete error table, latex:\\n'+errors.to_latex())\n",
    "errors.to_csv(out_prefix + 'errors.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_alert_batch(data, max_samples=0):\n",
    "    \"\"\"\n",
    "    Encode and structures single alerts similarly to pairs\n",
    "\n",
    "    Similar to _get_batch, but ommitting second alerts and correlation.\n",
    "    Currently, wont do batching.\n",
    "    \"\"\"\n",
    "    data = shuffle(data)\n",
    "    if max_samples:\n",
    "        data = data.head(max_samples)\n",
    "    alerts = np.array(data['encoded_alert'].values.tolist())\n",
    "    masks = np.array(data['mask'].values.tolist())\n",
    "    incidents = np.array(data['incident'].values.tolist())\n",
    "    return alerts, masks, incidents\n",
    "\n",
    "def precompute_distance_matrix(X):\n",
    "    \"\"\"\n",
    "    precomputing takes 20 sec/500 samples on js3, OMP_NUM_THREADS=16\n",
    "    precomputing takes 9 min/2632 samples on js3, OMP_NUM_THREADS=16\n",
    "    \"\"\"\n",
    "    with Timer('Precomputing distances for {} samples'.format(len(X))):\n",
    "        precomp_dist = np.zeros(shape=(len(X), len(X)))\n",
    "        for i in range(len(X)):\n",
    "            for j in range(len(X)):\n",
    "                precomp_dist[i, j] = sp.spatial.distance.cosine(X[i], X[j])\n",
    "    return precomp_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info('Getting alerts for clustering')\n",
    "clust_alerts = {\n",
    "    'train': _get_alert_batch(alerts_train),\n",
    "    'validation': _get_alert_batch(alerts_val),\n",
    "}\n",
    "\n",
    "logger.info(\"Precomputing clustering alert distances\")\n",
    "\n",
    "X = dict()\n",
    "X_dist_precomp = dict()\n",
    "y = dict()\n",
    "for (cut, v) in clust_alerts.items():\n",
    "    alerts_matrix, masks_matrix, incidents_vector = v\n",
    "    X[cut] = alert_to_vector(alerts_matrix, masks_matrix)\n",
    "    X_dist_precomp[cut] = precompute_distance_matrix(X[cut])\n",
    "    y[cut] = incidents_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cut in y.keys():\n",
    "    logger.info(\"Breakdown of {} labels:\\n\".format(cut) +  break_down_data(y[cut]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn import metrics\n",
    "\n",
    "def cluster(eps, min_samples, X_dist_precomp):\n",
    "    return DBSCAN(\n",
    "        eps=eps, min_samples=min_samples, metric='precomputed'\n",
    "    ).fit(X_dist_precomp)\n",
    "\n",
    "def build_cluster_to_incident_mapper(y, y_pred):\n",
    "    # Assign label to clusters according which incident has the largest part of its alert in the given cluster\n",
    "    # weight to handle class skew\n",
    "    weights = {l: 1/cnt for (l, cnt) in zip(*np.unique(y, return_counts=True))}\n",
    "    allocs = zip(y, y_pred)\n",
    "\n",
    "    from collections import Counter\n",
    "    c = Counter(map(tuple, allocs))\n",
    "\n",
    "    mapper = dict()\n",
    "    for _, (incident, cluster) in sorted([(c[k]*weights[k[0]], k) for k in c.keys()]):\n",
    "        mapper[cluster] = incident\n",
    "\n",
    "    mapper[-1] = -1 # Don't rely on what DBSCAN deems as noise\n",
    "    return mapper\n",
    "\n",
    "def dbscan_predict(dbscan_model, X_new, metric=sp.spatial.distance.cosine):\n",
    "    # Result is noise by default\n",
    "    y_new = np.ones(shape=len(X_new), dtype=int)*-1 \n",
    "\n",
    "    # Iterate all input samples for a label\n",
    "    for j, x_new in enumerate(X_new):\n",
    "        for i, x_core in enumerate(X['train'][dbscan_model.core_sample_indices_]): \n",
    "            if  metric(x_new, x_core) < dbscan_model.eps:\n",
    "                # Assign label of x_core to x_new\n",
    "                y_new[j] = dbscan_model.labels_[dbscan_model.core_sample_indices_[i]]\n",
    "                break\n",
    "    return y_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Iterating clustering algorithm parameters\")\n",
    "epss = np.array([0.0001, 0.0003, 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1])\n",
    "min_sampless = np.array([1, 3, 10, 30])\n",
    "\n",
    "def ParamSpaceMatrix(dtype=None):\n",
    "    return np.zeros(shape=(len(epss), len(min_sampless)), dtype=dtype)\n",
    "\n",
    "cl_model = ParamSpaceMatrix(dtype=object)\n",
    "mapper = ParamSpaceMatrix(dtype=object)\n",
    "\n",
    "cuts = ['train', 'validation']\n",
    "\n",
    "def ParamSpaceMatrices(dtype=None):\n",
    "    return {cut : ParamSpaceMatrix(dtype=dtype) for cut in cuts}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster and build mapper\n",
    "for i, eps in enumerate(epss):\n",
    "    for j, min_samples in enumerate(min_sampless):\n",
    "        logger.info(\"Clustering, eps={}, min_samples={}\".format(eps, min_samples))\n",
    "        # Cluster\n",
    "        cl_model[i,j] = cluster(eps, min_samples, X_dist_precomp['train'])\n",
    "        # get cluster assignments\n",
    "        y_pred = cl_model[i,j].labels_ \n",
    "        # Build classifier - get mapper used for classification\n",
    "        mapper[i,j] = build_cluster_to_incident_mapper(y['train'], y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict \n",
    "y_pred = ParamSpaceMatrices(dtype=object)\n",
    "y_pred_inc = ParamSpaceMatrices(dtype=object)\n",
    "\n",
    "for cut in cuts:\n",
    "    for i, eps in enumerate(epss):\n",
    "        for j, min_samples in enumerate(min_sampless):\n",
    "            if cut == 'train':\n",
    "                # pred is abused to hold clustering results\n",
    "                y_pred[cut][i,j] = cl_model[i,j].labels_ # cluster assignment\n",
    "                y_pred_inc[cut][i,j] = y[cut] # true incident label\n",
    "            elif cut == 'validation':\n",
    "                logger.info('Predicting for (eps, min_samples)=({:1.0e},{:>2d})'.format(eps, min_samples))\n",
    "                y_pred[cut][i,j] = dbscan_predict(cl_model[i][j], X[cut])\n",
    "                y_pred_inc[cut][i,j] = np.array([mapper[i,j][el] for el in y_pred[cut][i,j]]) # predict incident\n",
    "            else:\n",
    "                raise NotImplementedError('Unexpected value for cut:{}'.format(cut))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def false_alert_rate_outliers_score(y, y_pred):\n",
    "    idx_outliers = y_pred == -1\n",
    "    return (y[idx_outliers] == -1).mean()\n",
    "\n",
    "def false_alert_rate_clusters_score(y, y_pred):\n",
    "    idx_clusters = y_pred != -1\n",
    "    return (y[idx_clusters] == -1).mean()\n",
    "\n",
    "def arf_score(y, y_pred):\n",
    "    return len(y) / len(set(y_pred))\n",
    "\n",
    "def narf_score(y, y_pred):\n",
    "    return (len(y) / len(set(y_pred)) - 1) / (len(y) - 1)\n",
    "\n",
    "def imr_score(y, y_pred):\n",
    "    df_clustering = pd.DataFrame({\n",
    "        'cluster': y_pred,\n",
    "        'inc_true': y,\n",
    "    })\n",
    "    cluster_sizes = pd.DataFrame({'cluster_size': df_clustering[df_clustering.cluster != -1].groupby('cluster').size()})\n",
    "    df_clustering = pd.merge(df_clustering, cluster_sizes.reset_index(), on='cluster', how='outer')\n",
    "    # asuming one alert is picked at random from each cluster;\n",
    "    # probability that given alert is picked to represent the cluster it belongs to\n",
    "    df_clustering['alert_pick_prob'] = 1/df_clustering.cluster_size \n",
    "    # probability distribution of what incident a cluster will be asumed to represent\n",
    "    df_prob = pd.DataFrame(df_clustering.groupby(['cluster', 'inc_true']).sum().alert_pick_prob.rename('inc_hit'))\n",
    "    # probability that a given incident will not come out of a cluster\n",
    "    df_prob['inc_miss'] = 1 - df_prob.inc_hit.fillna(0)\n",
    "    assert (df_prob[df_prob.inc_miss < 0].inc_miss.abs() < 1e-12).all(), \"Error larger than 1e-12, still just imprecission?\"\n",
    "    df_prob = df_prob.abs()\n",
    "    # ... of any cluster\n",
    "    inc_miss_prob = df_prob.reset_index().groupby('inc_true').inc_miss.prod().rename('inc_miss_prob')\n",
    "    inc_miss_prob = inc_miss_prob[inc_miss_prob.index != -1] # Don't care about missing the noise pseudo-incident\n",
    "    return inc_miss_prob.sum() / df_clustering[df_clustering.inc_true != -1].inc_true.unique().shape[0]\n",
    "\n",
    "def cm_inc_clust(y, y_pred):\n",
    "    cm_inc_clust = pd.DataFrame(\n",
    "        metrics.confusion_matrix(y, y_pred),\n",
    "        index=sorted(set.union(set(y), set(y_pred))),\n",
    "        columns=sorted(set.union(set(y), set(y_pred))),\n",
    "    )\n",
    "    # drop dummy row for non-existing incident IDs\n",
    "    assert (cm_inc_clust.drop(list(set(y)), axis=0) == 0).as_matrix().all(), \"Non-empty row for invalid incident id\"\n",
    "    cm_inc_clust = cm_inc_clust.loc[sorted(list(set(y)))]\n",
    "\n",
    "    # drop dummy collumns for non-existing cluster IDs\n",
    "    assert (cm_inc_clust.drop(list(set(y_pred)), axis=1) == 0).as_matrix().all(), \"Non-empty collumn for invalid cluster id\"\n",
    "    cm_inc_clust = cm_inc_clust[sorted(list(set(y_pred)))]\n",
    "\n",
    "    cm_inc_clust.rename(index={-1: 'benign'}, inplace=True)\n",
    "    cm_inc_clust.rename(columns={-1: 'noise'}, inplace=True)\n",
    "    return cm_inc_clust\n",
    "\n",
    "def cm_inc_inc(y, y_pred_inc):\n",
    "    cm_inc_inc = pd.DataFrame(\n",
    "        metrics.confusion_matrix(y, y_pred_inc),\n",
    "        index=sorted(set.union(set(y), set(y_pred_inc))),\n",
    "        columns=sorted(set.union(set(y), set(y_pred_inc))),\n",
    "    )\n",
    "\n",
    "    cm_inc_inc.rename(index={-1: 'benign'}, inplace=True)\n",
    "    cm_inc_inc.rename(columns={-1: 'benign'}, inplace=True)\n",
    "    return cm_inc_inc\n",
    "\n",
    "def per_class_metrics(y, y_pred_inc):\n",
    "    d = dict(\n",
    "        zip(\n",
    "            ['precission', 'recall', 'f1', 'support'],\n",
    "            metrics.precision_recall_fscore_support(y, y_pred_inc)\n",
    "        )\n",
    "    )\n",
    "    cmat = metrics.confusion_matrix(y, y_pred_inc)\n",
    "    d['accuracy'] = cmat.diagonal()/cmat.sum(axis=1)\n",
    "    d['labels'] = sklearn.utils.multiclass.unique_labels(y, y_pred_inc)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating metrics\n",
    "m = {\n",
    "    # clustering\n",
    "    'n_clusters': ParamSpaceMatrices(dtype=int),\n",
    "    'homogenity': ParamSpaceMatrices(),\n",
    "    'outliers': ParamSpaceMatrices(dtype=int),\n",
    "    # classification, general\n",
    "    'accuracy': ParamSpaceMatrices(),\n",
    "    'precision': ParamSpaceMatrices(),\n",
    "    'recall': ParamSpaceMatrices(),\n",
    "    'f1': ParamSpaceMatrices(),\n",
    "    'per_class': ParamSpaceMatrices(dtype=object),\n",
    "    # correlating and filtering metrics\n",
    "    'arf': ParamSpaceMatrices(),\n",
    "    'narf': ParamSpaceMatrices(),\n",
    "    'imr': ParamSpaceMatrices(),\n",
    "    'faro': ParamSpaceMatrices(),\n",
    "    'farc': ParamSpaceMatrices(),\n",
    "    # confusion matrices\n",
    "    'cm_inc_clust': ParamSpaceMatrices(dtype=object),\n",
    "    'cm_inc_inc': ParamSpaceMatrices(dtype=object),\n",
    "}\n",
    "\n",
    "for cut in cuts:\n",
    "    for i, eps in enumerate(epss):\n",
    "        for j, min_samples in enumerate(min_sampless):\n",
    "            # clustering metrics\n",
    "            # Number of clusters in labels, ignoring noise if present.\n",
    "            m['n_clusters'][cut][i,j] = len(set(y_pred[cut][i,j])) - (1 if -1 in y_pred[cut][i,j] else 0)\n",
    "            m['outliers'][cut][i,j] = sum(y_pred[cut][i,j] == -1)\n",
    "            m['homogenity'][cut][i,j] = metrics.homogeneity_score(y[cut], y_pred[cut][i,j])\n",
    "            # classification metrics\n",
    "            m['accuracy'][cut][i,j] = metrics.accuracy_score(y[cut], y_pred_inc[cut][i,j])\n",
    "            m['precision'][cut][i,j] = metrics.precision_score(y[cut], y_pred_inc[cut][i,j], average='weighted')\n",
    "            m['recall'][cut][i,j] = metrics.recall_score(y[cut], y_pred_inc[cut][i,j], average='weighted')\n",
    "            m['f1'][cut][i,j] = metrics.f1_score(y[cut], y_pred_inc[cut][i,j], average='weighted')\n",
    "            m['per_class'][cut][i,j] = per_class_metrics(y[cut], y_pred_inc[cut][i,j])\n",
    "            # correlating and filtering\n",
    "            m['arf'][cut][i,j] = arf_score(y[cut], y_pred[cut][i,j])\n",
    "            m['narf'][cut][i,j] = narf_score(y[cut], y_pred[cut][i,j])\n",
    "            m['imr'][cut][i,j] = imr_score(y[cut], y_pred[cut][i,j])\n",
    "            m['faro'][cut][i,j] = false_alert_rate_outliers_score(y[cut], y_pred[cut][i,j])\n",
    "            m['farc'][cut][i,j] = false_alert_rate_clusters_score(y[cut], y_pred[cut][i,j])\n",
    "            # confusion matrices\n",
    "            m['cm_inc_clust'][cut][i,j] = cm_inc_clust(y[cut], y_pred[cut][i,j])\n",
    "            m['cm_inc_inc'][cut][i,j] = cm_inc_inc(y[cut], y_pred_inc[cut][i,j])\n",
    "            \n",
    "            logger.info(\n",
    "                \"Performance on {} cut with (eps, min_samples)=({:1.0e},{:>2d}): n_clusters={:>3d}, homogenity={:1.3f}, f1={:1.3f}, noise={:>3d}\".format(\n",
    "                    cut, eps, min_samples, \n",
    "                    m['n_clusters'][cut][i,j],\n",
    "                    m['homogenity'][cut][i,j],\n",
    "                    m['f1'][cut][i,j],\n",
    "                    m['outliers'][cut][i,j],\n",
    "                )\n",
    "            )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(out_prefix + 'metrics.pickle', 'w') as f:\n",
    "    pickle.dump(m, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_plot_prepare(\n",
    "    title,\n",
    "):\n",
    "    fig, ax = plt.subplots(figsize=(10,6))\n",
    "    ax.set_title(title)\n",
    "\n",
    "    ax.set_yscale('log')\n",
    "    ax.set_ylabel('min_samples')\n",
    "    ax.set_yticklabels(min_sampless)\n",
    "    ax.set_yticks(min_sampless)\n",
    "    ax.set_ylim(min_sampless[0]/3, min_sampless[-1]*3)\n",
    "\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_xlabel('eps')\n",
    "    ax.set_xticklabels(epss)\n",
    "    ax.set_xticks(epss)\n",
    "    ax.set_xlim(epss[0]/3,epss[-1]*3)\n",
    "    ax.xaxis.set_major_formatter(matplotlib.ticker.FormatStrFormatter('%.0e'))\n",
    "\n",
    "\n",
    "def param_plot_scatter(\n",
    "    data,\n",
    "    xcoords,\n",
    "    ycoords,\n",
    "\n",
    "):\n",
    "    # scaling\n",
    "    data = np.copy(data)\n",
    "    data = data-data.min() # Range to start at zero\n",
    "    data = data/data.max() # Range to end at one\n",
    "\n",
    "    coords = np.array([\n",
    "            (i, j)\n",
    "            for i in range(data.shape[0])\n",
    "            for j in range(data.shape[1])\n",
    "    ])\n",
    "\n",
    "    plt.scatter(\n",
    "        xcoords[coords[:,0]],\n",
    "        ycoords[coords[:,1]],\n",
    "        data[coords[:,0], coords[:,1]]*1000,\n",
    "        c='white',\n",
    "        alpha=0.5\n",
    "    )\n",
    "\n",
    "\n",
    "def param_plot_annotate(\n",
    "    data,\n",
    "    xcoords,\n",
    "    ycoords,\n",
    "    fmt='{}',\n",
    "):\n",
    "    coords = np.array([\n",
    "            (i, j)\n",
    "            for i in range(data.shape[0])\n",
    "            for j in range(data.shape[1])\n",
    "    ])\n",
    "        \n",
    "    for x, y, label in zip(\n",
    "        xcoords[coords[:,0]],\n",
    "        ycoords[coords[:,1]],\n",
    "        data[coords[:,0], coords[:,1]],\n",
    "    ):\n",
    "        plt.annotate(\n",
    "            fmt.format(label),\n",
    "            xy = (x, y), xytext = (0, 0),\n",
    "            textcoords = 'offset points', ha = 'center', va = 'center',\n",
    "        )\n",
    "\n",
    "\n",
    "def param_plot_save(filename):\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filename, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label, values, fmt in [\n",
    "    ('Cluster count', m['n_clusters'], '{}'),\n",
    "    ('Cluster homogenity', m['homogenity'], '{:.2f}'),\n",
    "    ('Outliers', m['outliers'], '{}'),\n",
    "]:\n",
    "    for cut in cuts:\n",
    "        param_plot_prepare('{} by DBSCAN parameters ({} alerts)'.format(label, cut))\n",
    "        param_plot_scatter(values[cut], epss, min_sampless)\n",
    "        param_plot_annotate(values[cut], epss, min_sampless, fmt=fmt)\n",
    "        \n",
    "        param_plot_save(\n",
    "            '{}{}_{}.pdf'.format(\n",
    "                out_prefix,\n",
    "                label.replace('(','').replace(')','').replace(\" \", \"_\").lower(),\n",
    "                cut\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label, values, fmt in [\n",
    "    ('Incident prediction accuracy', m['accuracy'], '{:.2f}'),\n",
    "    ('Incident prediction precision', m['precision'],  '{:.2f}'),\n",
    "    ('Incident prediction recall', m['recall'], '{:.2f}'),\n",
    "    ('Incident prediction F1 score', m['f1'], '{:.2f}'),\n",
    "]:\n",
    "    for cut in ['validation']: # training evaluated on true labels for clustering is of little use\n",
    "        param_plot_prepare('{} by DBSCAN parameters ({} alerts)'.format(label, cut))\n",
    "        param_plot_scatter(values[cut], epss, min_sampless)\n",
    "        param_plot_annotate(values[cut], epss, min_sampless, fmt=fmt)\n",
    "        \n",
    "        param_plot_save(\n",
    "            '{}{}_{}.pdf'.format(\n",
    "                out_prefix,\n",
    "                label.replace('(','').replace(')','').replace(\" \", \"_\").lower(),\n",
    "                cut\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label, values, fmt in [\n",
    "    ('Alert Reduction Factor', m['arf'], '{:.2f}'),\n",
    "    ('Normalised Alert Reduction Factor', m['narf'], '{:.2e}'),\n",
    "    ('Incident Miss Rate', m['imr'], '{:.2e}'),\n",
    "    ('False alerts rate among outliers', m['faro'], '{:.2f}'),\n",
    "]:\n",
    "    for cut in cuts:\n",
    "        param_plot_prepare('{} by DBSCAN parameters ({} alerts)'.format(label, cut))\n",
    "        param_plot_scatter(values[cut], epss, min_sampless)\n",
    "        param_plot_annotate(values[cut], epss, min_sampless, fmt=fmt)\n",
    "        \n",
    "        param_plot_save(\n",
    "            '{}{}_{}.pdf'.format(\n",
    "                out_prefix,\n",
    "                label.replace('(','').replace(')','').replace(\" \", \"_\").lower(),\n",
    "                cut\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info('One fold of cross validation completed')\n",
    "sys.exit(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering - test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if env['TEST_MS'] and env['TEST_EPS']:\n",
    "    logger.info('Continuing to use test data (eps={}, min_samples={})'.format(\n",
    "            env['TEST_MS'], env['TEST_EPS'],\n",
    "    ))\n",
    "else:\n",
    "    logger.info('Validation results completed, exiting')\n",
    "    sys.exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = env['TEST_EPS']\n",
    "min_samples = env['TEST_MS']\n",
    "i = epss.tolist().index(eps)\n",
    "j = min_sampless.tolist().index(min_samples)\n",
    "\n",
    "logger.info(\"Applying clusters to test data\")\n",
    "\n",
    "alerts_matrix, masks_matrix, incidents_vector = clust_alerts_test\n",
    "X = alert_to_vector(alerts_matrix, masks_matrix)\n",
    "y = incidents_vector\n",
    "\n",
    "y_pred = dbscan_predict(cl_model[i][j], X)\n",
    "y_pred_inc = np.array([mapper[i][j][el] for el in y_pred])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(set.union(set(y), set(y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Incident (i) to cluster (j) \\\"confusion matrix\\\":\\n\" + cm_inc_clust.to_string())\n",
    "logger.debug(\"Incident (i) to cluster (j) \\\"confusion matrix\\\" in latex:\\n\" + cm_inc_clust.to_latex())\n",
    "cm_inc_clust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Incident (i) to Incident (j) confusion matrix:\\n\" + cm_inc_inc.to_string())\n",
    "logger.debug(\"Incident (i) to Incident (j) confusion matrix in latex:\\n\" + cm_inc_inc.to_latex())\n",
    "cm_inc_inc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = metrics.confusion_matrix(y, y_pred_inc)\n",
    "logger.info(\"Classification accuracy: {:.2f}%\".format(\n",
    "    cm_inc_inc.as_matrix().diagonal().sum() / cm_inc_inc.as_matrix().sum() * 100\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = pd.DataFrame(\n",
    "    dict(zip(\n",
    "        ['precision', 'recall', 'f1-score', 'support'],\n",
    "        metrics.precision_recall_fscore_support(y, y_pred_inc),\n",
    "    )),\n",
    "    index=sorted(list(set(y))),\n",
    ")[['precision', 'recall', 'f1-score', 'support']]\n",
    "mean = report.mean(axis=0).drop('support')\n",
    "zum = report.sum(axis=0).drop(['precision', 'recall', 'f1-score'])\n",
    "\n",
    "report.loc['mean'] = mean\n",
    "report.loc['sum'] = zum\n",
    "report['support'] = report['support'].fillna(-1).astype(int)\n",
    "\n",
    "logger.info(\"Classification report:\\n\"+report.to_string())\n",
    "logger.debug(\"Classification report in latex:\\n\"+report.to_latex())\n",
    "report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info('Testing completed, exiting')\n",
    "sys.exit(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniq_counts(l, sort_key=itemgetter(1), sort_reverse=True):\n",
    "    try:\n",
    "        l = [json.dumps(el) for el in l]\n",
    "        logger.debug('dumped to json')\n",
    "    except:\n",
    "        l = [str(el) for el in l]\n",
    "        logger.warn('failed to dump to json, casting to str')\n",
    "    unique, unique_counts = np.unique(l, return_counts=True)\n",
    "    res = sorted(\n",
    "        zip(unique, unique_counts),\n",
    "        key=sort_key,\n",
    "        reverse=sort_reverse,\n",
    "    )\n",
    "    unique, count = map(list, zip(*res))\n",
    "    try:\n",
    "        unique = list(map(json.loads, unique))\n",
    "        logger.debug('read from json')\n",
    "    except:\n",
    "        logger.warn('failed to read as json, returning as str')\n",
    "    return unique, count\n",
    "\n",
    "# a bogus incident to hold all the unclassifiable test alerts\n",
    "noise_alerts = alerts_matrix[y_pred_inc == -1]\n",
    "noise_masks = masks_matrix[y_pred_inc == -1]\n",
    "\n",
    "def decode(alert, mask):\n",
    "    alert = alert[mask.astype(bool)] # apply mask\n",
    "    alert = [chr(c) for c in alert]\n",
    "    return ''.join(alert)\n",
    "\n",
    "noise_incident = (None, [decode(a, m) for a, m in zip(noise_alerts, noise_masks)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info('Counts of different noise alerts')\n",
    "uniq, counts = uniq_counts(\n",
    "    pool(modify(\n",
    "            [noise_incident],\n",
    "            [mask_ips, mask_tss, mask_ports],\n",
    ")))\n",
    "for i, (u, c) in enumerate(zip(uniq, counts)):\n",
    "    logger.info(\"{:2d}. n={:2d}: {}\".format(i+1, c, u[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info('Priority among noise alerts:')\n",
    "uniq, counts = uniq_counts(pool(modify(\n",
    "            [noise_incident],\n",
    "            [extract_prio],\n",
    ")), sort_key=itemgetter(0), sort_reverse=False)\n",
    "for u, c in zip(uniq, counts):\n",
    "    logger.info(\"Priority {}, n={:3d}, {:5.2f}%\".format(u[1], c, c/sum(counts)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info('Priority of all alerts:')\n",
    "all_alerts = pool(modify(\n",
    "        incidents,\n",
    "        [extract_prio],\n",
    "))\n",
    "all_alerts = [a[1] for a in all_alerts] # discard incident id\n",
    "uniq, counts = uniq_counts(\n",
    "    all_alerts, \n",
    "    sort_key=itemgetter(0), sort_reverse=False,\n",
    ")\n",
    "for u, c in zip(uniq, counts):\n",
    "    logger.info(\"Priority {}, n={:4d}, {:5.2f}%\".format(u, c, c/sum(counts)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info('Priority of all alerts by incident:')\n",
    "uniq, counts = uniq_counts(pool(modify(\n",
    "        incidents,\n",
    "        [extract_prio],\n",
    "    )),\n",
    "    sort_key=itemgetter(0), sort_reverse=False,\n",
    ")\n",
    "uniq = np.array(uniq)\n",
    "counts = np.array(counts)\n",
    "cnt_array = np.zeros([uniq[:,0].max(), uniq[:,1].max()], dtype=int)\n",
    "uniq = uniq-1 # incident and prio: 1-indexed, arrays: 0-indexed\n",
    "for (i, p), c in zip(uniq, counts):\n",
    "    cnt_array[i,p] = c\n",
    "\n",
    "s = \"Counts:\\n\"\n",
    "fmt = '{:<11}' + '{:>8}'*cnt_array.shape[1] + '\\n'\n",
    "s += fmt.format('', 'Prio 1', 'Prio 2', 'Prio 3')\n",
    "for i in np.arange(cnt_array.shape[0]):\n",
    "    s += fmt.format(*['Incident '+str(i+1)]+list(cnt_array[i,:]))\n",
    "logger.info(s)\n",
    "\n",
    "s = \"Normalised pr. incident:\\n\"\n",
    "fmt = '{:<11}' + '{:>8}'*cnt_array.shape[1] + '\\n'\n",
    "s += fmt.format('', 'Prio 1', 'Prio 2', 'Prio 3')\n",
    "fmt = '{:<11}' + '{:7.2f}%'*cnt_array.shape[1] + '\\n'\n",
    "for i in np.arange(cnt_array.shape[0]):\n",
    "    s += fmt.format(*['Incident '+str(i+1)]+list((100*cnt_array[i,:]/cnt_array.sum(axis=1)[i])))\n",
    "logger.info(s)\n",
    "\n",
    "s = \"Normalised across incident:\\n\"\n",
    "fmt = '{:<11}' + '{:>8}'*cnt_array.shape[1] + '\\n'\n",
    "s += fmt.format('', 'Prio 1', 'Prio 2', 'Prio 3')\n",
    "fmt = '{:<11}' + '{:7.2f}%'*cnt_array.shape[1] + '\\n'\n",
    "for i in np.arange(cnt_array.shape[0]):\n",
    "    s += fmt.format(*['Incident '+str(i+1)]+list(100*cnt_array[i,:]/cnt_array.sum()))\n",
    "logger.info(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
