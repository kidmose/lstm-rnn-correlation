{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alert correlation with a Long Short-Term Memmroy (LSTM) Recurrent Neural Network(RNN) and cosine similarity\n",
    "\n",
    "$\\cos(\\theta) = {A \\cdot B \\over \\|A\\| \\|B\\|} = \\frac{ \\sum\\limits_{i=1}^{n}{A_i \\times B_i} }{ \\sqrt{\\sum\\limits_{i=1}^{n}{(A_i)^2}} \\times \\sqrt{\\sum\\limits_{i=1}^{n}{(B_i)^2}} }$\n",
    "\n",
    "Huang, Po-Sen, et al. \"Learning deep structured semantic models for web search using clickthrough data.\" Proceedings of the 22nd ACM international conference on Conference on information & knowledge management. ACM, 2013.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (C) Egon Kidmose 2015-2017\n",
    "\n",
    "This file is part of lstm-rnn-correlation.\n",
    "\n",
    "lstm-rnn-correlation is free software: you can redistribute it and/or\n",
    "modify it under the terms of the GNU Lesser General Public License as\n",
    "published by the Free Software Foundation, either version 3 of the\n",
    "License, or (at your option) any later version.\n",
    "\n",
    "lstm-rnn-correlation is distributed in the hope that it will be\n",
    "useful, but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n",
    "Lesser General Public License for more details.\n",
    "\n",
    "You should have received a copy of the GNU Lesser General Public\n",
    "License along with lstm-rnn-correlation. If not, see\n",
    "<http://www.gnu.org/licenses/>.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import lasagne\n",
    "from lasagne.layers import *\n",
    "from lasagne.objectives import binary_crossentropy, squared_error\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "%matplotlib nbagg\n",
    "\n",
    "NUM_INPUTS = 2**7 # Size of ASCII\n",
    "NUM_UNITS_ENC = 10\n",
    "\n",
    "# For testing\n",
    "TEST_ALERT = 'abcd'\n",
    "TEST_BATCH = [[ord(c) for c in TEST_ALERT]]*5\n",
    "TEST_SHAPE = (5, 4)\n",
    "X_ALERT, X_ALERT_1, X_ALERT_2 = T.imatrices('test_in_1', 'test_in_2', 'test_in_3', )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Input layers - does nothing\n",
    "l_in_1 = InputLayer((None, None), name=\"Input layer 1\")\n",
    "l_in_2 = InputLayer((None, None), name=\"Input layer 2\")\n",
    "\n",
    "# Test\n",
    "test_res = lasagne.layers.get_output(l_in_1, inputs={l_in_1: X_ALERT}).eval(\n",
    "    {X_ALERT: TEST_BATCH})\n",
    "assert (test_res == TEST_BATCH).all(), \"Unexpected output\"\n",
    "\n",
    "print(test_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Embedding layers - applies one hot encoding\n",
    "l_emb_1 = EmbeddingLayer(l_in_1, NUM_INPUTS, NUM_INPUTS, \n",
    "                         W=np.eye(NUM_INPUTS,dtype='float32'),\n",
    "                         name='Embedding layer 1')\n",
    "l_emb_2 = EmbeddingLayer(l_in_2, NUM_INPUTS, NUM_INPUTS, \n",
    "                         W=np.eye(NUM_INPUTS,dtype='float32'),\n",
    "                         name='Embedding layer 2')\n",
    "l_emb_1.params[l_emb_1.W].remove('trainable') # Fix weight\n",
    "l_emb_2.params[l_emb_2.W].remove('trainable') # Fix weight\n",
    "\n",
    "# Test\n",
    "test_res = lasagne.layers.get_output(l_emb_1, inputs={l_in_1: X_ALERT}).eval(\n",
    "    {X_ALERT: TEST_BATCH})\n",
    "assert (np.argmax(test_res, axis=2) == TEST_BATCH).all()\n",
    "assert np.all(test_res.shape == (TEST_SHAPE[0], TEST_SHAPE[1], NUM_INPUTS))\n",
    "\n",
    "print(np.argmax(test_res, axis=2))\n",
    "\n",
    "print(test_res.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# LSTM layers - two LSTM layers sharing weights etc. \n",
    "# Gates are to be shared between the two input paths \n",
    "# - they parse inputs from the same source after all.\n",
    "\n",
    "# Instantiate gates acc. to lasagne.layers.LSTMLayer defaults\n",
    "ingate = lasagne.layers.Gate()\n",
    "forgetgate = lasagne.layers.Gate()\n",
    "cell = lasagne.layers.Gate(\n",
    "    W_cell=None, \n",
    "    nonlinearity=lasagne.nonlinearities.tanh #\n",
    ")\n",
    "outgate = lasagne.layers.Gate()\n",
    "\n",
    "l_enc_1 = LSTMLayer(l_emb_1, \n",
    "                    num_units=NUM_UNITS_ENC,\n",
    "                    ingate=ingate, forgetgate=forgetgate,\n",
    "                    cell=cell, outgate=outgate,\n",
    "                    name='LSTM layer 1',\n",
    "                   )\n",
    "l_enc_2 = LSTMLayer(l_emb_2, \n",
    "                    num_units=NUM_UNITS_ENC,\n",
    "                    ingate=ingate, forgetgate=forgetgate,\n",
    "                    cell=cell, outgate=outgate,\n",
    "                    name='LSTM layer 2',\n",
    "                   )\n",
    "\n",
    "# Test \n",
    "test_res_1 = lasagne.layers.get_output(l_enc_1, inputs={l_in_1: X_ALERT}).eval(\n",
    "    {X_ALERT: TEST_BATCH})\n",
    "test_res_2 = lasagne.layers.get_output(l_enc_2, inputs={l_in_2: X_ALERT}).eval(\n",
    "    {X_ALERT: TEST_BATCH})\n",
    "assert test_res_1.shape == (TEST_SHAPE[0], TEST_SHAPE[1], NUM_UNITS_ENC), \"Unexpected dimensions\"\n",
    "# TODO: For some reason the above fails to tie the weights together, so the below line fails.\n",
    "# As a work arround we'll just supply all input pairs in swapped order\n",
    "# assert np.all(test_res_1 == test_res_2) , \"The two inputs lines differ\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Slice Layer\n",
    "# Pick the outputs for the last entry in the sequences. \n",
    "\n",
    "l_last_1 = SliceLayer(l_enc_1, indices=-1, axis=1, name=\"Slice layer 1\")\n",
    "l_last_2 = SliceLayer(l_enc_2, indices=-1, axis=1, name=\"Slice layer 2\")\n",
    "\n",
    "# Test\n",
    "test_res_1 = lasagne.layers.get_output(l_last_1, inputs={l_in_1: X_ALERT}).eval(\n",
    "    {X_ALERT: TEST_BATCH})\n",
    "assert test_res_1.shape == (TEST_SHAPE[0], NUM_UNITS_ENC)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Cosine similarity layer implementation\n",
    "class CosineSimilarityLayer(MergeLayer):\n",
    "    \"\"\"Calculates the cosine of two inputs.\"\"\"\n",
    "    def __init__(self, incoming1, incoming2, **kwargs):\n",
    "        \"\"\"Instantiates the layer with incoming1 and incoming2 as the inputs.\"\"\"\n",
    "        incomings = [incoming1, incoming2]\n",
    "        \n",
    "        for incoming in incomings:\n",
    "            if isinstance(incoming, tuple):\n",
    "                if len(incoming) != 2:\n",
    "                    raise NotImplementedError(\"Requires shape to be exactly (BATCH_SIZE, N).\")\n",
    "            elif len(incoming.output_shape) != 2:\n",
    "                raise NotImplementedError(\"Requires shape to be exactly (BATCH_SIZE, N).\")\n",
    "                \n",
    "        super(CosineSimilarityLayer, self).__init__(incomings, **kwargs)\n",
    "    \n",
    "    def get_output_shape_for(self, input_shapes):\n",
    "        \"\"\"Return output shape: (batch_size, 1).\"\"\"\n",
    "        if len(input_shapes) != 2:\n",
    "            raise ValueError(\"Requires exactly 2 input_shapes\")\n",
    "\n",
    "        for input_shape in input_shapes:\n",
    "            if len(input_shape) != 2:\n",
    "                raise NotImplementedError(\"Requires shape to be exactly (BATCH_SIZE, N).\")\n",
    "\n",
    "        return (input_shape[0],)\n",
    "    \n",
    "    def get_output_for(self, inputs, **kwargs):\n",
    "        \"\"\"Calculates the cosine similarity.\"\"\"\n",
    "        nominator = (inputs[0] * inputs[1]).sum(axis=1)\n",
    "        return nominator\n",
    "        denominator = T.sqrt((inputs[0]**2).sum(axis=1)) * T.sqrt((inputs[1]**2).sum(axis=1))\n",
    "        return nominator/denominator\n",
    "        \n",
    "# Test\n",
    "test_in_1 = InputLayer((None, None))\n",
    "test_in_2 = InputLayer((None, None))\n",
    "test_layer = CosineSimilarityLayer(test_in_1, test_in_2)\n",
    "in1, in2 = T.dmatrices('in1', 'in2')\n",
    "\n",
    "test_res = lasagne.layers.get_output(test_layer, inputs={\n",
    "        test_in_1: in1,\n",
    "        test_in_2: in2\n",
    "    }).eval({\n",
    "        in1: [[0, 1], [1, 0], [0, -1]],\n",
    "        in2: [[0, 1], [0, 1], [0, 1]],\n",
    "    })\n",
    "assert len(test_res.shape) == len(test_layer.output_shape), \"Dimension mismatch\"\n",
    "assert (test_res == [ 1.,  0., -1.]).all(), \"Invalid output\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Cosine similarity layer\n",
    "l_cos = CosineSimilarityLayer(l_last_1, l_last_2, name=\"Cosine similarity layer\")\n",
    "\n",
    "# Test\n",
    "test_res = lasagne.layers.get_output(l_cos, inputs={\n",
    "        l_in_1: X_ALERT_1, \n",
    "        l_in_2: X_ALERT_2, \n",
    "    }).eval({\n",
    "        X_ALERT_1: TEST_BATCH,\n",
    "        X_ALERT_2: TEST_BATCH,\n",
    "    })\n",
    "assert test_res.shape == (TEST_SHAPE[0],)\n",
    "network = l_cos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Loss\n",
    "input_1, input_2 = T.imatrices(\"Symbolic input 1\", \"Symbolic input 2\")\n",
    "truth = T.dvector(\"Symbolic truth\")\n",
    "prediction = get_output(\n",
    "    network, \n",
    "    inputs={l_in_1: input_1, l_in_2: input_2},\n",
    "    name=\"Prediction function\",\n",
    "    )\n",
    "loss = squared_error(prediction, truth).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train  \n",
    "# Heavily inspired by mnist.py example\n",
    "params = lasagne.layers.get_all_params(network, trainable=True)\n",
    "updates = lasagne.updates.nesterov_momentum(\n",
    "    loss,\n",
    "    get_all_params([network], trainable=True),\n",
    "    learning_rate=0.01, \n",
    "    momentum=0.9, \n",
    ")\n",
    "train_func = theano.function(\n",
    "    [input_1, input_2, truth], \n",
    "    [loss, prediction], \n",
    "    updates=updates,\n",
    "    name=\"Training function\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Test\n",
    "truncated_prediction = (T.gt(prediction, 0)*2-1).astype('int32')\n",
    "acc = T.mean(T.eq(truncated_prediction, truth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create dummy data\n",
    "N_DUMMY_SAMPLES = 100000\n",
    "N_ASCII = 2**7\n",
    "ALERT_LENGTH = 100\n",
    "\n",
    "cor = np.random.randint(0, 2, (N_DUMMY_SAMPLES))*2-1\n",
    "alerts_1 = np.random.randint(\n",
    "    0, N_ASCII, \n",
    "    (N_DUMMY_SAMPLES, ALERT_LENGTH),\n",
    ").astype('int32')\n",
    "alerts_2 = np.random.randint(\n",
    "    0, N_ASCII, \n",
    "    (N_DUMMY_SAMPLES, ALERT_LENGTH),\n",
    ").astype('int32')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# perform training\n",
    "NUM_EPOCHS = 100\n",
    "from datetime import datetime\n",
    "\n",
    "n = int(N_DUMMY_SAMPLES/NUM_EPOCHS)\n",
    "for e in range(NUM_EPOCHS):\n",
    "    start = datetime.now()\n",
    "    loss, _ = train_func(\n",
    "        alerts_1[e*n:(e+1)*n],\n",
    "        alerts_2[e*n:(e+1)*n],\n",
    "        cor[e*n:(e+1)*n],\n",
    "    )\n",
    "    print(\"Epoch {} completed, loss: {}, duration: {}\".format(e, loss, datetime.now()-start))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
