{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alert correlation with a Long Short-Term Memmroy (LSTM) Recurrent Neural Network(RNN) and cosine similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (C) Egon Kidmose 2015-2017\n",
    "\n",
    "This file is part of lstm-rnn-correlation.\n",
    "\n",
    "lstm-rnn-correlation is free software: you can redistribute it and/or\n",
    "modify it under the terms of the GNU Lesser General Public License as\n",
    "published by the Free Software Foundation, either version 3 of the\n",
    "License, or (at your option) any later version.\n",
    "\n",
    "lstm-rnn-correlation is distributed in the hope that it will be\n",
    "useful, but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n",
    "Lesser General Public License for more details.\n",
    "\n",
    "You should have received a copy of the GNU Lesser General Public\n",
    "License along with lstm-rnn-correlation. If not, see\n",
    "<http://www.gnu.org/licenses/>.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import lasagne\n",
    "from lasagne.layers import *\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "%matplotlib nbagg\n",
    "\n",
    "NUM_INPUTS = 2**7 # Size of ASCII\n",
    "NUM_UNITS_ENC = 10\n",
    "\n",
    "# For testing\n",
    "TEST_ALERT = 'abcd'\n",
    "TEST_BATCH = [[ord(c) for c in TEST_ALERT]]*5\n",
    "TEST_SHAPE = (5, 4)\n",
    "X_ALERT = T.imatrix()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Input layers - does nothing\n",
    "l_in_1 = InputLayer((None, None))\n",
    "l_in_2 = InputLayer((None, None))\n",
    "\n",
    "test_res = lasagne.layers.get_output(l_in_1, inputs={l_in_1: X_ALERT}).eval(\n",
    "    {X_ALERT: TEST_BATCH})\n",
    "assert (test_res == TEST_BATCH).all(), \"Unexpected output\"\n",
    "\n",
    "print(test_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Embedding layers - applies one hot encoding\n",
    "l_emb_1 = EmbeddingLayer(l_in_1, NUM_INPUTS, NUM_INPUTS, \n",
    "                         W=np.eye(NUM_INPUTS,dtype='float32'),\n",
    "                         name='Embedding 1')\n",
    "l_emb_2 = EmbeddingLayer(l_in_2, NUM_INPUTS, NUM_INPUTS, \n",
    "                         W=np.eye(NUM_INPUTS,dtype='float32'),\n",
    "                         name='Embedding 2')\n",
    "l_emb_1.params[l_emb_1.W].remove('trainable') # Fix weight\n",
    "l_emb_2.params[l_emb_2.W].remove('trainable') # Fix weight\n",
    "\n",
    "# Test\n",
    "test_res = lasagne.layers.get_output(l_emb_1, inputs={l_in_1: X_ALERT}).eval(\n",
    "    {X_ALERT: TEST_BATCH})\n",
    "assert (np.argmax(test_res, axis=2) == TEST_BATCH).all()\n",
    "assert np.all(test_res.shape == (TEST_SHAPE[0], TEST_SHAPE[1], NUM_INPUTS))\n",
    "\n",
    "print(np.argmax(test_res, axis=2))\n",
    "\n",
    "print(test_res.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# LSTM layers - two LSTM layers sharing weights etc. \n",
    "# Gates are to be shared between the two input paths \n",
    "# - they parse inputs from the same source after all.\n",
    "\n",
    "# Instantiate gates acc. to lasagne.layers.LSTMLayer defaults\n",
    "ingate = lasagne.layers.Gate()\n",
    "forgetgate = lasagne.layers.Gate()\n",
    "cell = lasagne.layers.Gate(\n",
    "    W_cell=None, \n",
    "    nonlinearity=lasagne.nonlinearities.tanh #\n",
    ")\n",
    "outgate = lasagne.layers.Gate()\n",
    "\n",
    "l_enc_1 = LSTMLayer(l_emb_1, \n",
    "                    num_units=NUM_UNITS_ENC,\n",
    "                    ingate=ingate, forgetgate=forgetgate,\n",
    "                    cell=cell, outgate=outgate,\n",
    "                    name='LSTM 1',\n",
    "                   )\n",
    "l_enc_2 = LSTMLayer(l_emb_2, \n",
    "                    num_units=NUM_UNITS_ENC,\n",
    "                    ingate=ingate, forgetgate=forgetgate,\n",
    "                    cell=cell, outgate=outgate,\n",
    "                    name='LSTM 2',\n",
    "                   )\n",
    "\n",
    "# Test \n",
    "test_res_1 = lasagne.layers.get_output(l_enc_1, inputs={l_in_1: X_ALERT}).eval(\n",
    "    {X_ALERT: TEST_BATCH})\n",
    "test_res_2 = lasagne.layers.get_output(l_enc_2, inputs={l_in_2: X_ALERT}).eval(\n",
    "    {X_ALERT: TEST_BATCH})\n",
    "assert test_res_1.shape == (TEST_SHAPE[0], TEST_SHAPE[1], NUM_UNITS_ENC), \"Unexpected dimensions\"\n",
    "# TODO: For some reason the above fails to tie the weights together, so the below line fails.\n",
    "# As a work arround we'll just supply all input pairs in swapped order\n",
    "# assert np.all(test_res_1 == test_res_2) , \"The two inputs lines differ\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Slice Layer\n",
    "# Pick the outputs for the last entry in the sequences. \n",
    "\n",
    "l_last_1 = lasagne.layers.SliceLayer(l_enc_1, indices=-1, axis=1)\n",
    "l_last_2 = lasagne.layers.SliceLayer(l_enc_2, indices=-1, axis=1)\n",
    "\n",
    "# Test\n",
    "test_res_1 = lasagne.layers.get_output(l_last_1, inputs={l_in_1: X_ALERT}).eval(\n",
    "    {X_ALERT: TEST_BATCH})\n",
    "assert test_res_1.shape == (TEST_SHAPE[0], NUM_UNITS_ENC)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Cosine layer definition\n",
    "class CosineSimilarityLayer(MergeLayer):\n",
    "    \"\"\"Calculates the cosine of two inputs.\"\"\"\n",
    "    def __init__(self, incoming1, incoming2, **kwargs):\n",
    "        \"\"\"Instantiates the layer with incoming1 and incoming2 as the inputs.\"\"\"\n",
    "        incomings = [incoming1, incoming2]\n",
    "        \n",
    "        for incoming in incomings:\n",
    "            if isinstance(incoming, tuple):\n",
    "                if len(incoming) != 2:\n",
    "                    raise NotImplementedError(\"Requires shape to be exactly (BATCH_SIZE, N).\")\n",
    "            elif len(incoming.output_shape) != 2:\n",
    "                raise NotImplementedError(\"Requires shape to be exactly (BATCH_SIZE, N).\")\n",
    "                \n",
    "        super(CosineSimilarityLayer, self).__init__(incomings, **kwargs)\n",
    "    \n",
    "    def get_output_shape_for(self, input_shapes):\n",
    "        \"\"\"Return output shape: (batch_size, 1).\"\"\"\n",
    "        if len(input_shapes) != 2:\n",
    "            raise ValueError(\"Requires exactly 2 input_shapes\")\n",
    "\n",
    "        for input_shape in input_shapes:\n",
    "            if len(input_shape) != 2:\n",
    "                raise NotImplementedError(\"Requires shape to be exactly (BATCH_SIZE, N).\")\n",
    "\n",
    "        return (input_shape[0],)\n",
    "    \n",
    "    def get_output_for(self, inputs, **kwargs):\n",
    "        \"\"\"Calculates the cosine similarity.\"\"\"\n",
    "        nominator = (inputs[0] * inputs[1]).sum(axis=1)\n",
    "        return nominator\n",
    "        denominator = T.sqrt((inputs[0]**2).sum(axis=1)) * T.sqrt((inputs[1]**2).sum(axis=1))\n",
    "        return nominator/denominator\n",
    "        \n",
    "# Test\n",
    "test_in_1 = InputLayer((None, None))\n",
    "test_in_2 = InputLayer((None, None))\n",
    "test_layer = CosineSimilarityLayer(test_in_1, test_in_2)\n",
    "in1, in2 = T.dmatrices('in1', 'in2')\n",
    "\n",
    "test_res = lasagne.layers.get_output(test_layer, inputs={\n",
    "        test_in_1: in1,\n",
    "        test_in_2: in2\n",
    "    }).eval({\n",
    "        in1: [[0, 1], [1, 0], [0, -1]],\n",
    "        in2: [[0, 1], [0, 1], [0, 1]],\n",
    "    })\n",
    "assert len(test_res.shape) == len(test_layer.output_shape), \"Dimension mismatch\"\n",
    "assert (test_res == [ 1.,  0., -1.]).all(), \"Invalid output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
