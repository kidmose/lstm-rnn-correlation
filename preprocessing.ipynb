{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Copyright (C) Egon Kidmose 2015-2017\n",
    "\n",
    "This file is part of lstm-rnn-correlation.\n",
    "\n",
    "lstm-rnn-correlation is free software: you can redistribute it and/or\n",
    "modify it under the terms of the GNU Lesser General Public License as\n",
    "published by the Free Software Foundation, either version 3 of the\n",
    "License, or (at your option) any later version.\n",
    "\n",
    "lstm-rnn-correlation is distributed in the hope that it will be\n",
    "useful, but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n",
    "Lesser General Public License for more details.\n",
    "\n",
    "You should have received a copy of the GNU Lesser General Public\n",
    "License along with lstm-rnn-correlation. If not, see\n",
    "<http://www.gnu.org/licenses/>.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import numpy as np\n",
    "import re\n",
    "import datetime\n",
    "import ipaddress\n",
    "import socket\n",
    "import os\n",
    "\n",
    "import lstm_rnn_tied_weights\n",
    "\n",
    "import logging\n",
    "logging.getLogger().handlers = []\n",
    "logger = lstm_rnn_tied_weights.logger\n",
    "runid = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S-\") + socket.gethostname()\n",
    "out_dir = 'output/alert-merging/' + runid\n",
    "if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir)\n",
    "out_prefix = out_dir + '/' + runid + '-'\n",
    "# info log file\n",
    "infofh = logging.FileHandler(out_prefix + 'info.log')\n",
    "infofh.setLevel(logging.INFO)\n",
    "infofh.setFormatter(logging.Formatter(\n",
    "        fmt='%(message)s',\n",
    "))\n",
    "logger.addHandler(infofh)\n",
    "# verbose log file\n",
    "vfh = logging.FileHandler(out_prefix + 'verbose.log')\n",
    "vfh.setLevel(logging.DEBUG)\n",
    "vfh.setFormatter(logging.Formatter(\n",
    "        fmt='%(asctime)s - PID:%(process)d - %(levelname)s - %(message)s',\n",
    "))\n",
    "logger.addHandler(vfh)\n",
    "logger.info('Output prefix: '+ out_prefix)\n",
    "\n",
    "# matplotlib\n",
    "try: # might, might not have x available\n",
    "    import os\n",
    "    os.environ['DISPLAY']\n",
    "except KeyError:\n",
    "    import matplotlib\n",
    "    matplotlib.use('Agg')\n",
    "try: # might, might not be notebook\n",
    "    %matplotlib inline\n",
    "except NameError:\n",
    "    pass\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "seed = 1468586473 # Unix time at time of writing\n",
    "def rndseed():\n",
    "    global seed\n",
    "    seed += 1\n",
    "    return seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "IPV4 = '(?:[0-9]{1,3}(?:\\.[0-9]{1,3}){3})'\n",
    "IPV6 = '(?:[0-9a-f]|:){1,4}(?::(?:[0-9a-f]{0,4})*){1,7}'\n",
    "IP = '(?:{}|{})'.format(IPV4, IPV6)\n",
    "IP_PORT = '('+IP+')(?::([^ ]+))?'\n",
    "\n",
    "SNORT_REGEX = re.compile('^(.*)  \\[\\*\\*] \\[([^]]*)] (.*) \\[Priority: ([0-9])] {([^}]*)} ('+IP+')(?::([^ ]+))? -> ('+IP+')(?::([^ ]+))?\\n')\n",
    "SNORT_TS_FMT = '%m/%d/%y-%H:%M:%S.%f'\n",
    "SNORT_TS_FMT_NO_YR = '%m/%d-%H:%M:%S.%f'\n",
    "\n",
    "def strptime(string):\n",
    "    ts = None\n",
    "    try:\n",
    "        ts = datetime.datetime.strptime(string, SNORT_TS_FMT)\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        ts = datetime.datetime.strptime(string, SNORT_TS_FMT_NO_YR)\n",
    "    except:\n",
    "        pass\n",
    "    if ts is None:\n",
    "        raise Exception('Failed to parse {}: {}'.format(type(string), string))\n",
    "    return ts\n",
    "\n",
    "def strftime(ts):\n",
    "    return ts.strftime(SNORT_TS_FMT)\n",
    "\n",
    "def parse_line(line):\n",
    "    tupl = re.match(SNORT_REGEX, line).groups()\n",
    "    tupl = tuple([strptime(tupl[0])]) + tupl[1:]\n",
    "    return tupl\n",
    "\n",
    "\n",
    "def build_line(tupl):\n",
    "    tupl = tuple(tupl) # pandas.core.series.Series doesn't add like tuple \n",
    "    tupl = tuple([strftime(tupl[0])]) + tupl[1:]\n",
    "    return \"{}  [**] [{}] {} [Priority: {}] {{{}}} {}:{} -> {}:{}\\n\".format(*tupl)\n",
    "\n",
    "test_code =\\\n",
    "\"\"\"\n",
    "for fn in data['filename']:\n",
    "    for l in open(fn).readlines():\n",
    "        p = parse_line(l)\n",
    "        b = build_line(p)\n",
    "        assert l == b or l == (b[:5]+b[8:]), \"Mismatch in result\"\n",
    "\"\"\"\n",
    "\n",
    "test_row = pd.Series((\n",
    "        pd.tslib.Timestamp('2016-05-04 09:38:44.365433'),\n",
    "        '129:12:1',\n",
    "        'Consecutive TCP small segments exceeding threshold [**] [Classification: Potentially Bad Traffic]',\n",
    "        '2',\n",
    "        'TCP',\n",
    "        '10.149.34.24',\n",
    "        '445',\n",
    "        '10.130.8.48',\n",
    "        '60741',\n",
    "))\n",
    "desired_output = '05/04/16-09:38:44.365433  [**] [129:12:1] '+\\\n",
    "    'Consecutive TCP small segments exceeding threshold [**] [Classification: Potentially Bad Traffic] '+\\\n",
    "    '[Priority: 2] {TCP} 10.149.34.24:445 -> 10.130.8.48:60741\\n'\n",
    "\n",
    "assert build_line(test_row) == desired_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logger.info('Loading data')\n",
    "from data_cfg import data\n",
    "# load files\n",
    "data_tmp = DataFrame()\n",
    "for index, row in data.iterrows():\n",
    "    print(\"Opening: {}\".format(row['filename']))\n",
    "    with open(row['filename']) as f:\n",
    "        alerts = DataFrame(\n",
    "            map(parse_line, f.readlines()),\n",
    "            columns=['ts', 'rid', 'msg', 'prio', 'proto', 'srcip', 'srcport', 'dstip', 'dstport']\n",
    "        )\n",
    "        alerts['incident'] = row['incident']   \n",
    "        data_tmp = data_tmp.append(pd.merge(data, alerts))\n",
    "        print(\"Loaded: {}\".format(row['filename']))\n",
    "data = data_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "difficult_ips = [\n",
    "    '94.63.149.152',\n",
    "    '147.32.84.165',\n",
    "]\n",
    "for ip in difficult_ips:\n",
    "    assert re.match('('+IP+')', ip).group()\n",
    "\n",
    "difficult_ips_port = [\n",
    "    '94.63.149.152:80',\n",
    "    '147.32.84.165:1040',\n",
    "]\n",
    "for ip in difficult_ips_port:\n",
    "    res = re.match(IP_PORT, ip).groups()\n",
    "    assert res is not None\n",
    "    assert len(res) == 2\n",
    "\n",
    "difficult_lines = [\n",
    "    '08/15-15:53:48.900440  [**] [120:3:1] (http_inspect) NO CONTENT-LENGTH OR TRANSFER-ENCODING IN HTTP RESPONSE [**] [Classification: Unknown Traffic] [Priority: 3] {TCP} 94.63.149.152:80 -> 147.32.84.165:1040\\n',\n",
    "]\n",
    "for l in difficult_lines:\n",
    "    res = parse_line(l)\n",
    "    assert len(res) == 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing\n",
    "\n",
    "## data overview by incident"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_data_overview(data):\n",
    "    df_inc_cnt = DataFrame(data.groupby(['incident']).size().rename('inc_cnt').reset_index())\n",
    "    df_inc_cnt['inc_cnt_pct'] = df_inc_cnt['inc_cnt']/data.count()[0]*100\n",
    "\n",
    "    df_inc_prio_cnt = DataFrame(\n",
    "        pd.merge(data, df_inc_cnt, on='incident').groupby(['incident', 'prio', 'inc_cnt']\n",
    "    ).size().rename('inc_prio_cnt').reset_index())\n",
    "    df_inc_prio_cnt['inc_prio_cnt_pct'] = df_inc_prio_cnt['inc_prio_cnt']/df_inc_prio_cnt['inc_cnt']*100\n",
    "\n",
    "    df_overview = pd.merge(df_inc_cnt, df_inc_prio_cnt).groupby(['incident', 'prio']).first().reset_index()\n",
    "\n",
    "    tot_prio = data.groupby(['prio']).size().rename('inc_prio_cnt')\n",
    "    df_tot = DataFrame(tot_prio)\n",
    "    df_tot['inc_prio_cnt_pct'] = df_tot['inc_prio_cnt']/data.count()[0]*100\n",
    "    df_tot['inc_cnt'] = data.count()[0]\n",
    "    df_tot['inc_cnt_pct'] = 100\n",
    "    df_tot['incident'] = 'total'\n",
    "\n",
    "    df_overview = pd.concat([df_overview.reset_index(), df_tot.reset_index()])\\\n",
    "        .groupby(['incident', 'inc_cnt', 'inc_cnt_pct', 'prio', ])\\\n",
    "        .first().drop('index', 1)\n",
    "\n",
    "    return df_overview\n",
    "\n",
    "logger.info('Overview of unprocessed data')\n",
    "df_overview = get_data_overview(data)\n",
    "logger.debug(df_overview.to_latex())\n",
    "logger.debug(df_overview.to_string())\n",
    "df_overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discard true alerts found in the false alert set\n",
    "The set of false alerts also contains true alerts, i.e. wrong label.   \n",
    "We strive to discard those.  \n",
    "Discarding is done according to lists of bad IP addresses. If either source or destination of an alert match it is discarded.\n",
    "\n",
    "### Comercial solution\n",
    "A commercial detection solution provides a list of IPs that was found to have been infected with malicious software.\n",
    "\n",
    "Discarding might introduce a bias - this effect needs to be investigated.  \n",
    "To do this we look at distribution of priority and distribution of rule IDs after discarding.  \n",
    "This is compared to the same prior to discarding. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logger.info('Discard by commercial')\n",
    "\n",
    "def srcip_or_dstip_in_row(row, ip_list):\n",
    "    return row['srcip'] in ip_list or \\\n",
    "        row['dstip'] in ip_list\n",
    "\n",
    "test_srcip = '94.63.149.152'\n",
    "test_dstip ='147.32.84.165'\n",
    "test_row = pd.Series({'srcip':test_srcip, 'dstip':test_dstip})\n",
    "assert srcip_or_dstip_in_row(test_row, [test_srcip])\n",
    "assert srcip_or_dstip_in_row(test_row, [test_dstip])\n",
    "assert srcip_or_dstip_in_row(test_row, [test_srcip, test_srcip])\n",
    "assert not srcip_or_dstip_in_row(test_row, list())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('data/own-recordings/commercial-discard-ips.csv') as f:\n",
    "    ips_discard_commercial = f.readlines()\n",
    "    ips_discard_commercial = map(str.strip, ips_discard_commercial)\n",
    "\n",
    "idx_disc_com = data.apply(\n",
    "    lambda row : srcip_or_dstip_in_row(row, ips_discard_commercial),\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "assert data[((data['incident']!='benign') & (idx_disc_com))].size == 0, \"non-benign should not be discarded\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# on the benign group before discarding\n",
    "idx_benign = data['incident'] == 'benign'\n",
    "benign_prio_size = data[idx_benign].groupby('prio').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logger.info('Priority distribution before and after discard by commercial:')\n",
    "disc_com_prio_size = data[idx_benign & (idx_disc_com == False)].groupby('prio').size()\n",
    "df_prio_before_after_comm = pd.concat(\n",
    "    [\n",
    "        benign_prio_size.rename('Count, all benign'), \n",
    "        (benign_prio_size/benign_prio_size.sum()*100).rename('Percentage, all benign'),\n",
    "        disc_com_prio_size.rename('Count, after discard by commercial'),\n",
    "        (disc_com_prio_size/disc_com_prio_size.sum()*100).rename('Percentage, after discard by commercial'),\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "logger.debug(df_prio_before_after_comm.to_latex())\n",
    "logger.debug(df_prio_before_after_comm.to_string())\n",
    "df_prio_before_after_comm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logger.info(\n",
    "    \"Unique rule IDs in orignal benign: %d\" % \n",
    "    data[idx_benign].groupby('rid').size().size\n",
    ")\n",
    "logger.info(\n",
    "    \"Unique rule IDs in alerts discarded by comercial: %d\" % \n",
    "    data[idx_benign & idx_disc_com].groupby('rid').size().size\n",
    ")\n",
    "logger.info(\n",
    "    \"Unique rule IDs after discarding: %d\" % \n",
    "    data[idx_benign & (idx_disc_com == False)].groupby('rid').size().size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual inspection\n",
    "Heuristics are applied, aiming to find the most likely true alerts, and they are manually investigated.  \n",
    "A list is produced, consisting of all IPs in alerts that was matched by heuristics, that could no be confirmed to be false.\n",
    "\n",
    "The same analysis of bias as above is performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logger.info('Discard by manual')\n",
    "with open('data/own-recordings/manual-discard-ips.csv') as f:\n",
    "    ips_discard_manual = f.readlines()\n",
    "    ips_discard_manual = map(str.strip, ips_discard_manual)\n",
    "\n",
    "idx_disc_man = data.apply(\n",
    "    lambda row : srcip_or_dstip_in_row(row, ips_discard_manual),\n",
    "    axis=1,\n",
    ")\n",
    "assert data[((data['incident']!='benign') & (idx_disc_man))].size == 0, \"non-benign should not be discarded\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Priority distribution before and after discard by manual:')\n",
    "disc_man_prio_size = data[idx_benign & (idx_disc_man == False)].groupby('prio').size()\n",
    "df_prio_before_after_manual = pd.concat(\n",
    "    [\n",
    "        benign_prio_size.rename('Count, all benign'), \n",
    "        (benign_prio_size/benign_prio_size.sum()*100).rename('Percentage, all benign'),\n",
    "        disc_man_prio_size.rename('Count, after discard by manual'),\n",
    "        (disc_man_prio_size/disc_man_prio_size.sum()*100).rename('Percentage, after discard by manual'),\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "logger.debug(df_prio_before_after_manual.to_latex())\n",
    "logger.debug(df_prio_before_after_manual.to_string())\n",
    "df_prio_before_after_manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logger.info(\n",
    "    \"Unique rule IDs in orignal benign: %d\" % \n",
    "    data[idx_benign].groupby('rid').size().size\n",
    ")\n",
    "logger.info(\n",
    "    \"Unique rule IDs in alerts discarded by manual: %d\" % \n",
    "    data[idx_benign & idx_disc_man].groupby('rid').size().size\n",
    ")\n",
    "logger.info(\n",
    "    \"Unique rule IDs after discarding: %d\" % \n",
    "    data[idx_benign & (idx_disc_man == False)].groupby('rid').size().size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual and commercial, effects of discarding both "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Priority distribution before and after discard:')\n",
    "disc_prio_size = data[idx_benign & (idx_disc_com == False) & (idx_disc_man == False)].groupby('prio').size()\n",
    "df_prio_before_after = pd.concat(\n",
    "    [\n",
    "        benign_prio_size.rename('Count, all benign'), \n",
    "        (benign_prio_size/benign_prio_size.sum()*100).rename('Percentage, all benign'),\n",
    "        disc_prio_size.rename('Count, after discard'),\n",
    "        (disc_prio_size/disc_prio_size.sum()*100).rename('Percentage, after discard'),\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "logger.debug(df_prio_before_after.to_latex())\n",
    "logger.debug(df_prio_before_after.to_string())\n",
    "df_prio_before_after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logger.info(\n",
    "    \"Unique rule IDs in orignal benign: %d\" % \n",
    "    data[idx_benign].groupby('rid').size().size\n",
    ")\n",
    "logger.info(\n",
    "    \"Unique rule IDs in alerts discarded: %d\" % \n",
    "    data[idx_benign & (idx_disc_man | idx_disc_com)].groupby('rid').size().size\n",
    ")\n",
    "logger.info(\n",
    "    \"Unique rule IDs after discarding: %d\" % \n",
    "    data[idx_benign & ((idx_disc_man | idx_disc_com)==False)].groupby('rid').size().size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logger.info('Analysing amount of discard by rule ID')\n",
    "df_discard = pd.concat(\n",
    "    [\n",
    "        data[idx_benign].groupby('rid').size().rename('before'),\n",
    "        data[idx_benign & (idx_disc_com == False)].groupby('rid').size().rename('com'),\n",
    "        data[idx_benign & (idx_disc_man == False)].groupby('rid').size().rename('man'),\n",
    "        data[idx_benign & ((idx_disc_man | idx_disc_com)==False)].groupby('rid').size().rename('both'),\n",
    "    ],\n",
    "    axis=1,\n",
    ").fillna(0).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_discard['Commercial'] = (df_discard['before']-df_discard['com'])/df_discard['before']*100\n",
    "df_discard['Manual'] = (df_discard['before']-df_discard['man'])/df_discard['before']*100\n",
    "df_discard['Both'] = (df_discard['before']-df_discard['both'])/df_discard['before']*100\n",
    "\n",
    "logger.debug(df_discard.to_latex())\n",
    "logger.debug(df_discard.to_string())\n",
    "df_prio_before_after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ax = df_discard[['Commercial', 'Manual', 'Both']].plot(kind='bar', figsize=(20,10))\n",
    "ax.set_title('Percentage of all benign discarded under various strategies, by Rule ID')\n",
    "ax.set_xlabel('Rule ID')\n",
    "ax.set_ylabel('%')\n",
    "plt.tight_layout()\n",
    "plt.savefig(out_prefix+'discard_ruleid.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Discard\n",
    "data = data[((idx_disc_man | idx_disc_com)==False) | (idx_benign == False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge in time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# data time overview\n",
    "logger.info('Time overview, before processing')\n",
    "df_ts = data[['incident', 'ts']].groupby('incident').agg(['min', 'max'])['ts']\n",
    "df_ts.columns = ['start', 'stop', ]\n",
    "df_ts['dur'] = df_ts['stop']-df_ts['start']\n",
    "\n",
    "logger.debug(df_ts.to_latex())\n",
    "logger.debug(df_ts.to_string())\n",
    "df_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = None\n",
    "ax = None\n",
    "def time_span_plot(dataframe):\n",
    "    \"Dataframe must have 'start' and 'duration' keys\"\n",
    "    global fig, ax\n",
    "    fig, ax = plt.subplots()\n",
    "    for index, row in dataframe.iterrows():\n",
    "        x = row['start'] + row['dur']/2\n",
    "        y = int(index) if index is not 'benign' else 0\n",
    "        xerr = row['dur']/2\n",
    "        ax.errorbar(x, y, xerr=xerr)\n",
    "    \n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    \n",
    "    ax.yaxis.set_ticks_position('none')\n",
    "    ax.xaxis.set_ticks_position('bottom')\n",
    "    \n",
    "    ax.set_ylim(np.array(ax.get_ylim()) + [-1, 1])\n",
    "    \n",
    "    fig.set_size_inches(14, 2)\n",
    "\n",
    "time_span_plot(df_ts[(df_ts.index == 'benign')])\n",
    "plt.tight_layout()\n",
    "plt.savefig(out_prefix+'timespan_benign.pdf', bbox_inches='tight')\n",
    "\n",
    "time_span_plot(df_ts[(df_ts.index != 'benign')])\n",
    "plt.tight_layout()\n",
    "plt.savefig(out_prefix+'timespan_malicious.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# calculate random shift to end up within boundaries of benign\n",
    "logger.info('Calculating random shifts')\n",
    "new_start_min = df_ts.loc['benign']['start']\n",
    "new_start_max = df_ts.loc['benign']['stop']-df_ts['dur']\n",
    "window = new_start_max - new_start_min\n",
    "assert (np.array(\n",
    "        window+df_ts['dur']-df_ts.loc['benign']['dur'],\n",
    "        dtype='object') == 0).all(), \"window is wrong\"\n",
    "\n",
    "np.random.seed(rndseed())\n",
    "df_ts['shift'] = new_start_min - df_ts['start'] \\\n",
    "    + map(lambda delta : np.random.rand()*delta, window)\n",
    "\n",
    "logger.debug(df_ts.to_latex())\n",
    "logger.debug(df_ts.to_string())\n",
    "df_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logger.info('Applying time shift')\n",
    "\n",
    "# apply shift\n",
    "shifted_data = pd.merge(data, df_ts.reset_index(), on='incident')\n",
    "shifted_data['ts'] = shifted_data['ts'] + shifted_data['shift']\n",
    "assert shifted_data['ts'].max() == data.groupby(['incident']).max().loc['benign']['ts']\n",
    "assert shifted_data['ts'].min() == data.groupby(['incident']).min().loc['benign']['ts']\n",
    "data = shifted_data\n",
    "data = data.sort_values('ts')\n",
    "data = data[data_tmp.columns] # only original columns\n",
    "\n",
    "df_ts = data[['incident', 'ts']].groupby('incident').agg(['min', 'max'])['ts']\n",
    "df_ts.columns = ['start', 'stop', ]\n",
    "df_ts['dur'] = df_ts['stop']-df_ts['start']\n",
    "\n",
    "logger.debug(df_ts.to_latex())\n",
    "logger.debug(df_ts.to_string())\n",
    "df_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "time_span_plot(df_ts)\n",
    "ax.plot(new_start_min, 0, 'og')\n",
    "ax.plot(new_start_max[:-1], range(1, len(new_start_max)), 'or')\n",
    "plt.tight_layout()\n",
    "plt.savefig(out_prefix+'timespan_merged.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Merge in IP space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logger.info('Merging in IP address space')\n",
    "\n",
    "# rewrite victim IPs to match benign\n",
    "malicious_ips = data[data['victim_ip'] != 'benign']\\\n",
    "    [['incident', 'victim_ip']].drop_duplicates()\n",
    "\n",
    "data_benign = data[data['incident'] == 'benign']\n",
    "benign_ips = pd.Series(pd.concat([data_benign['srcip'],data_benign['dstip']]).unique(), name='benign_ips')\n",
    "def is_private_ip(string):\n",
    "    try:\n",
    "        return ipaddress.IPv4Address(string).is_private\n",
    "    except ipaddress.AddressValueError:\n",
    "        logger.info(\"ignoring IPv6 addres: {}\".format(string))\n",
    "        return False\n",
    "    \n",
    "benign_ips = benign_ips[np.array(map(is_private_ip, benign_ips))] # only private\n",
    "benign_ips = benign_ips.sample(malicious_ips.shape[0], random_state=rndseed())\n",
    "df_replace = DataFrame(\n",
    "    zip(malicious_ips['incident'], malicious_ips['victim_ip'], benign_ips),\n",
    "    columns=['incident', 'from_ip', 'to_ip']\n",
    ")\n",
    "\n",
    "logger.debug(df_replace.to_latex())\n",
    "logger.debug(df_replace.to_string())\n",
    "df_replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# perform IP replacement\n",
    "src_update = pd.merge(data.reset_index(), df_replace, left_on=['incident', 'srcip'], right_on=['incident', 'from_ip'])\n",
    "src_update['srcip'] = src_update['to_ip']\n",
    "src_update = src_update.drop(['from_ip', 'to_ip'], axis=1)\n",
    "src_update = src_update.set_index('index')\n",
    "data.update(src_update)\n",
    "\n",
    "dst_update = pd.merge(data.reset_index(), df_replace, left_on=['incident', 'dstip'], right_on=['incident', 'from_ip'])\n",
    "dst_update['dstip'] = dst_update['to_ip']\n",
    "dst_update = dst_update.drop(['from_ip', 'to_ip'], axis=1)\n",
    "dst_update = dst_update.set_index('index')\n",
    "data.update(dst_update)\n",
    "\n",
    "assert (np.array(src_update.groupby('incident').size()) > 0).all(), \"No src updated, highly suspicous\"\n",
    "assert (np.array(dst_update.groupby('incident').size()) > 0).all(), \"No dst updated, highly suspicous\"\n",
    "assert pd.merge(data, df_replace, left_on=['incident', 'srcip'], right_on=['incident', 'from_ip']).shape[0] == 0\n",
    "assert pd.merge(data, df_replace, left_on=['incident', 'dstip'], right_on=['incident', 'from_ip']).shape[0] == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reconstruct alerts\n",
    "data['alert'] = data[['ts', 'rid', 'msg', 'prio', 'proto', 'srcip', 'srcport', 'dstip', 'dstport']].apply(build_line, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stratify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logger.info('Stratifying data such that benign make up 50 %')\n",
    "\n",
    "malicious = data[(data['incident']!='benign')]\n",
    "benign = data[(data['incident']=='benign')]\n",
    "benign = benign.sample(n=len(malicious), random_state=rndseed())\n",
    "data = pd.concat([benign, malicious])\n",
    "\n",
    "# Shuffle\n",
    "np.random.seed(rndseed())\n",
    "data.reindex(np.random.permutation(data.index))\n",
    "\n",
    "logger.info('Overview of stratified data')\n",
    "df_overview = get_data_overview(data)\n",
    "logger.debug(df_overview.to_latex())\n",
    "logger.debug(df_overview.to_string())\n",
    "df_overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split alerts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logger.info('Splitting data')\n",
    "cut_sizes = np.array([60, 20, 20])\n",
    "limits = cut_sizes.cumsum()/cut_sizes.sum()\n",
    "np.random.seed(rndseed())\n",
    "data['cut'] = np.argmax(np.tile(np.random.rand(len(data)), (3, 1)).T < limits, axis=1)\n",
    "assert (\n",
    "    (data['cut'] == 0).astype(bool) ^ \\\n",
    "    (data['cut'] == 1).astype(bool) ^ \\\n",
    "    (data['cut'] == 2).astype(bool)\n",
    ").all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logger.info('Overview of cut: Training')\n",
    "df_overview_train = get_data_overview(data[data['cut']==0])\n",
    "logger.debug(df_overview_train.to_latex())\n",
    "logger.debug(df_overview_train.to_string())\n",
    "df_overview_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logger.info('Overview of cut: Validation')\n",
    "df_overview_val = get_data_overview(data[data['cut']==1])\n",
    "logger.debug(df_overview_val.to_latex())\n",
    "logger.debug(df_overview_val.to_string())\n",
    "df_overview_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logger.info('Overview of cut: Test')\n",
    "df_overview_test = get_data_overview(data[data['cut']==2])\n",
    "logger.debug(df_overview_test.to_latex())\n",
    "logger.debug(df_overview_test.to_string())\n",
    "df_overview_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save alerts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.to_csv(\n",
    "    'data/own-recordings/alerts-merged-cleaned.log.1465471791',\n",
    "    columns=['incident', 'alert', 'cut'],\n",
    "    index=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save test dataset of limited size\n",
    "data_test = DataFrame()\n",
    "for (i, c) in [(i, c) for i in data['incident'].unique() for c in data['cut'].unique()]:\n",
    "    data_test = pd.concat([\n",
    "        data_test,\n",
    "        data[(data['incident'] == i) & (data['cut'] == c)].head(100)\n",
    "    ])\n",
    "data_test.to_csv(\n",
    "    'data/own-recordings/alerts-merged-cleaned.log.1465471791.test',\n",
    "    columns=['incident', 'alert', 'cut'],\n",
    "    index=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
